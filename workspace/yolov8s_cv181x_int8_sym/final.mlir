#loc = loc(unknown)
#loc1 = loc("images")
#loc2 = loc("images/model.0/conv/Conv_output_0_Conv_si8")
module @yolov8s attributes {module.FLOPs = 28709930000 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv181x", module.cores = 1 : i64, module.devices = 1 : i64, module.inputs = ["images"], module.mode = "INT8", module.outputs = ["/model.22/Sigmoid_output_0_Sigmoid_f32", "/model.22/dfl/conv/Conv_output_0_Conv_f32"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.weight_file = "yolov8s_tpu_addressed_cv181x_int8_sym_weight.npz"} {
  module @yolov8s attributes {module.coeff_addr = 1099511627776 : i64, module.coeff_size = 11231792 : i64, module.device_id = 0 : i64, module.neuron_size = 4096000 : i64, module.private_size = 1228800 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x640x640xf32> loc(unknown)) -> (tensor<1x1x8400xf32, 5497558138880 : i64>, tensor<1x1x4x8400xf32, 4398046511104 : i64>) {
      %0 = "top.Input"(%arg0) {black_level = 1.120000e+02 : f64, channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = true, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "rgb", resize_dims = [640, 640], scale = [0.0039215688593685627, 0.0039215688593685627, 0.0039215688593685627], white_level = 4.095000e+03 : f64} : (tensor<1x3x640x640xf32>) -> tensor<1x3x640x640x!quant.calibrated<f32<-1.000000e+00:1.000000e+00>>, 3298534883328 : i64> loc(#loc1)
      %1 = call @subfunc_0(%0) : (tensor<1x3x640x640x!quant.calibrated<f32<-1.000000e+00:1.000000e+00>>, 3298534883328 : i64>) -> tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64> loc(#loc)
      %2:2 = call @subfunc_1(%1) : (tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64>) -> (tensor<1x1x8400xf32, 5497558138880 : i64>, tensor<1x1x4x8400xf32, 4398046511104 : i64>) loc(#loc)
      return %2#0, %2#1 : tensor<1x1x8400xf32, 5497558138880 : i64>, tensor<1x1x4x8400xf32, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x640x640x!quant.calibrated<f32<-1.000000e+00:1.000000e+00>>, 3298534883328 : i64> loc("images")) -> tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64> attributes {id = 0 : i64, mode = #tpu<run_mode CPU>, next_index = array<i32: 1>} {
      %0 = "tpu.GenericCpu"(%arg0) {cpu_op_name = "quant", param = {from = "FP32", scale = 1.270000e+02 : f32, to = "INT8"}} : (tensor<1x3x640x640x!quant.calibrated<f32<-1.000000e+00:1.000000e+00>>, 3298534883328 : i64>) -> tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64> loc(#loc2)
      return %0 : tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_1(%arg0: tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64> loc("images/model.0/conv/Conv_output_0_Conv_si8")) -> (tensor<1x1x8400xf32, 5497558138880 : i64>, tensor<1x1x4x8400xf32, 4398046511104 : i64>) attributes {id = 1 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099517768960 : i64> loc(#loc3)
      %2 = "top.Weight"() : () -> tensor<1x32x9x4xsi8, 1099517767808 : i64> loc(#loc4)
      %3 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514162832 : i64> loc(#loc5)
      %4 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099521361248 : i64> loc(#loc6)
      %5 = "top.Weight"() : () -> tensor<1x64x9x32xsi8, 1099512975184 : i64> loc(#loc7)
      %6 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517919840 : i64> loc(#loc8)
      %7 = "tpu.Group"(%arg0) ({
        %302 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12160, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 13, 29, 45, 61, 77, 93, 109, 125, 141, 157, 173, 189, 205, 221, 237, 253, 269, 285, 301, 317, 333, 349, 365, 381, 397, 413, 429, 445, 461, 477, 493, 509, 525, 541, 557, 573, 589, 605, 621], h_slice = [16, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19], w_idx = [0], w_slice = [640], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 4 : i64} : (tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64>) -> tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>> loc(#loc10)
        %303 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24064, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [4], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x4xsi8, 1099517767808 : i64>) -> tensor<1x32x9x4xsi8> loc(#loc11)
        %304 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12160, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099517768960 : i64>) -> tensor<1x32x1x9xi8> loc(#loc12)
        %305 = "tpu.Load"(%3) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24208, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514162832 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc13)
        %306 = "tpu.Conv2D"(%302, %303, %304) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 11520, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111, 119, 127, 135, 143, 151, 159, 167, 175, 183, 191, 199, 207, 215, 223, 231, 239, 247, 255, 263, 271, 279, 287, 295, 303, 311], h_slice = [8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], w_idx = [0], w_slice = [320], id = 4, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 4 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>>, tensor<1x32x9x4xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x320x320x!quant.uniform<i8:f32, 0.468737768503937>> loc(#loc14)
        %307 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 29696, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x32xsi8, 1099512975184 : i64>) -> tensor<1x64x9x32xsi8> loc(#loc15)
        %308 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12208, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099521361248 : i64>) -> tensor<1x64x1x9xi8> loc(#loc16)
        %309 = "tpu.Lut"(%306, %305) {ginfo = #tpu.lg<out_addr = 0, out_size = 11520, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103, 111, 119, 127, 135, 143, 151, 159, 167, 175, 183, 191, 199, 207, 215, 223, 231, 239, 247, 255, 263, 271, 279, 287, 295, 303, 311], h_slice = [8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], w_idx = [0], w_slice = [320], id = 7, stage = 1, group_type = 0>} : (tensor<1x32x320x320x!quant.uniform<i8:f32, 0.468737768503937>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x320x320x!quant.uniform<i8:f32, 0.43493836614173226>> loc(#loc17)
        %310 = "tpu.Load"(%6) {do_bcast = true, ginfo = #tpu.lg<out_addr = 23808, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517919840 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc18)
        %311 = "tpu.Conv2D"(%309, %307, %308) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [160], id = 9, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x320x320x!quant.uniform<i8:f32, 0.43493836614173226>>, tensor<1x64x9x32xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.68807983385826776>> loc(#loc19)
        %312 = "tpu.Lut"(%311, %310) {ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [160], id = 10, stage = 1, group_type = 0>} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.68807983385826776>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>> loc(#loc9)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [160], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>, 0 : i64> loc(#loc9)
        "tpu.Yield"(%313) : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>, 0 : i64>) -> () loc(#loc9)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, -4, 10, 0, 1, 2], group_type = 0 : i64, hsecs = 40 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-4, 1, 2], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x640x640x!quant.uniform<i8:f32, 0.0078740157480314959>, 2199023255552 : i64>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>, 0 : i64> loc(#loc9)
      %8 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099518001632 : i64> loc(#loc20)
      %9 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099516430288 : i64> loc(#loc21)
      %10 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517471952 : i64> loc(#loc22)
      %11 = "tpu.Group"(%7) ({
        %302 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152], h_slice = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [160], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>, 0 : i64>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>> loc(#loc24)
        %303 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10240, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099516430288 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc25)
        %304 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11008, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099518001632 : i64>) -> tensor<1x64x1x9xi8> loc(#loc26)
        %305 = "tpu.Load"(%10) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10752, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517471952 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc27)
        %306 = "tpu.Conv2D"(%302, %303, %304) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152], h_slice = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [160], id = 4, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.42168459527559055>> loc(#loc28)
        %307 = "tpu.Lut"(%306, %305) {ginfo = #tpu.lg<out_addr = 22528, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152], h_slice = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [160], id = 5, stage = 1, group_type = 0>} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.42168459527559055>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>> loc(#loc23)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 22528, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152], h_slice = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], w_idx = [0], w_slice = [160], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64> loc(#loc23)
        "tpu.Yield"(%308) : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64>) -> () loc(#loc23)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 20 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [2, 1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.71302666220472444>, 0 : i64>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64> loc(#loc23)
      %12 = "tpu.Slice"(%11, %0, %0, %0, %0) {axes = [], ends = [-1, 32, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64>, none, none, none, none) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64> loc(#loc29)
      %13 = "tpu.Slice"(%11, %0, %0, %0, %0) {axes = [], ends = [-1, 64, -1, -1], offset = [0, 32, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64>, none, none, none, none) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 2457600 : i64> loc(#loc30)
      %14 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x1x9xi8, 1099517472352 : i64> loc(#loc31)
      %15 = "top.Weight"() {do_compress = true} : () -> tensor<1x32x9x32xsi8, 1099514757584 : i64> loc(#loc32)
      %16 = "tpu.Conv2D"(%13, %15, %14) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 2457600 : i64>, tensor<1x32x9x32xsi8, 1099514757584 : i64>, tensor<1x32x1x9xi8, 1099517472352 : i64>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.30551811259842521>, 0 : i64> loc(#loc33)
      %17 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514799824 : i64> loc(#loc34)
      %18 = "top.Weight"() : () -> tensor<1x32x1x9xi8, 1099518053168 : i64> loc(#loc35)
      %19 = "top.Weight"() : () -> tensor<1x32x9x32xsi8, 1099522357856 : i64> loc(#loc36)
      %20 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521367584 : i64> loc(#loc37)
      %21 = "tpu.Group"(%16) ({
        %302 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149], h_slice = [11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11], w_idx = [0], w_slice = [160], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.30551811259842521>, 0 : i64>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.30551811259842521>> loc(#loc39)
        %303 = "tpu.Load"(%17) {do_bcast = true, ginfo = #tpu.lg<out_addr = 7936, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514799824 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc40)
        %304 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30976, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [32], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x9x32xsi8, 1099522357856 : i64>) -> tensor<1x32x9x32xsi8> loc(#loc41)
        %305 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 15872, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x9xi8, 1099518053168 : i64>) -> tensor<1x32x1x9xi8> loc(#loc42)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 9, 19, 29, 39, 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149], h_slice = [11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11], w_idx = [0], w_slice = [160], id = 4, stage = 1, group_type = 0>} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.30551811259842521>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.2259418968503937>> loc(#loc43)
        %307 = "tpu.Load"(%20) {do_bcast = true, ginfo = #tpu.lg<out_addr = 7680, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521367584 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc44)
        %308 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150], h_slice = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [160], id = 6, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.2259418968503937>>, tensor<1x32x9x32xsi8>, tensor<1x32x1x9xi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.25557005905511809>> loc(#loc45)
        %309 = "tpu.Lut"(%308, %307) {ginfo = #tpu.lg<out_addr = 24576, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150], h_slice = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [160], id = 7, stage = 1, group_type = 0>} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.25557005905511809>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>> loc(#loc38)
        %310 = "tpu.Store"(%309) {ginfo = #tpu.lg<out_addr = 24576, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150], h_slice = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [160], id = 8, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>, 3276800 : i64> loc(#loc38)
        "tpu.Yield"(%310) : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>, 3276800 : i64>) -> () loc(#loc38)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 8, -2, 6, 5, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 16 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-3, 1], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.30551811259842521>, 0 : i64>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>, 3276800 : i64> loc(#loc38)
      %22 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099517771552 : i64> loc(#loc46)
      %23 = "top.Weight"() : () -> tensor<1x64x1x96xsi8, 1099517958624 : i64> loc(#loc47)
      %24 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516879056 : i64> loc(#loc48)
      %25 = "tpu.Group"(%13, %21, %12) ({
        %302 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 2457600 : i64>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>> loc(#loc50)
        %303 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 1, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>, 3276800 : i64>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>> loc(#loc51)
        %304 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>> loc(#loc52)
        %305 = "tpu.Add"(%302, %303) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3840, buffer_addr = 25344, buffer_size = 3840, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 3, stage = 1, group_type = 0>, multipliers = [113, 102], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.26931933149606302>> loc(#loc53)
        %306 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x96xsi8, 1099517958624 : i64>) -> tensor<1x64x1x96xsi8> loc(#loc54)
        %307 = "tpu.Load"(%22) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12032, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099517771552 : i64>) -> tensor<1x64x1x9xi8> loc(#loc55)
        %308 = "tpu.Concat"(%304, %302, %305) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 11520, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 6, stage = 1, group_type = 0>, multipliers = [115, 115, 65], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 6]} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.26931933149606302>>) -> tensor<1x96x160x160x!quant.uniform<i8:f32, 0.26296856299212601>> loc(#loc56)
        %309 = "tpu.Load"(%24) {do_bcast = true, ginfo = #tpu.lg<out_addr = 29184, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516879056 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc57)
        %310 = "tpu.Conv2D"(%308, %306, %307) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 8, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x160x160x!quant.uniform<i8:f32, 0.26296856299212601>>, tensor<1x64x1x96xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.20622726535433072>> loc(#loc58)
        %311 = "tpu.Lut"(%310, %309) {ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 9, stage = 1, group_type = 0>} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.20622726535433072>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>> loc(#loc49)
        %312 = "tpu.Store"(%311) {ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144, 150, 155], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [160], id = 10, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>, 0 : i64> loc(#loc49)
        "tpu.Yield"(%312) : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>, 0 : i64>) -> () loc(#loc49)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 10, -2, 6, 4, 5, -3, 8, 7, -4, 9, 0, 1], group_type = 0 : i64, hsecs = 27 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-4, 2], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 2457600 : i64>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.215743762992126>, 3276800 : i64>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.23781815511811022>, 1638400 : i64>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>, 0 : i64> loc(#loc49)
      %26 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099516582992 : i64> loc(#loc59)
      %27 = "top.Weight"() : () -> tensor<1x128x9x64xsi8, 1099520871376 : i64> loc(#loc60)
      %28 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514011408 : i64> loc(#loc61)
      %29 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099518006560 : i64> loc(#loc62)
      %30 = "top.Weight"() : () -> tensor<1x128x1x128xsi8, 1099521343456 : i64> loc(#loc63)
      %31 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518049760 : i64> loc(#loc64)
      %32 = "tpu.Group"(%25) ({
        %302 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 0, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x64xsi8, 1099520871376 : i64>) -> tensor<1x128x9x64xsi8> loc(#loc66)
        %303 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 8960, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95, 101, 107, 113, 119, 125, 131, 137, 143, 149, 155], h_slice = [6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5], w_idx = [0], w_slice = [160], id = 1, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>, 0 : i64>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>> loc(#loc67)
        %304 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9472, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099516582992 : i64>) -> tensor<1x128x1x9xi8> loc(#loc68)
        %305 = "tpu.Load"(%28) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32512, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514011408 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc69)
        %306 = "tpu.Conv2D"(%303, %302, %304) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>>, tensor<1x128x9x64xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.08196778897637795>> loc(#loc70)
        %307 = "tpu.Load"(%30) {do_bcast = false, ginfo = #tpu.lg<out_addr = 21248, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xsi8, 1099521343456 : i64>) -> tensor<1x128x1x128xsi8> loc(#loc71)
        %308 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9616, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099518006560 : i64>) -> tensor<1x128x1x9xi8> loc(#loc72)
        %309 = "tpu.Lut"(%306, %305) {ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 7, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.08196778897637795>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066642506299212589>> loc(#loc73)
        %310 = "tpu.Load"(%31) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9216, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518049760 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc74)
        %311 = "tpu.Conv2D"(%309, %307, %308) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 9, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066642506299212589>>, tensor<1x128x1x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.096222118897637792>> loc(#loc75)
        %312 = "tpu.Lut"(%311, %310) {ginfo = #tpu.lg<out_addr = 28672, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 10, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.096222118897637792>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>> loc(#loc65)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 28672, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64> loc(#loc65)
        "tpu.Yield"(%313) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64>) -> () loc(#loc65)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, 0, -4, 10, 1, 2], group_type = 0 : i64, hsecs = 27 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [2], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12394116614173228>, 0 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64> loc(#loc65)
      %33 = "tpu.Slice"(%32, %0, %0, %0, %0) {axes = [], ends = [-1, 64, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64>, none, none, none, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64> loc(#loc76)
      %34 = "tpu.Slice"(%32, %0, %0, %0, %0) {axes = [], ends = [-1, 128, -1, -1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64>, none, none, none, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 3276800 : i64> loc(#loc77)
      %35 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099518052576 : i64> loc(#loc78)
      %36 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x64xsi8, 1099522535216 : i64> loc(#loc79)
      %37 = "tpu.Conv2D"(%34, %36, %35) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 3276800 : i64>, tensor<1x64x9x64xsi8, 1099522535216 : i64>, tensor<1x64x1x9xi8, 1099518052576 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.084716969291338581>, 409600 : i64> loc(#loc80)
      %38 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521925920 : i64> loc(#loc81)
      %39 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099518049184 : i64> loc(#loc82)
      %40 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099515721680 : i64> loc(#loc83)
      %41 = "tpu.Group"(%37) ({
        %302 = "tpu.Load"(%37) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 9, 19, 29, 39, 49, 59, 69], h_slice = [11, 12, 12, 12, 12, 12, 12, 11], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.084716969291338581>, 409600 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.084716969291338581>> loc(#loc85)
        %303 = "tpu.Load"(%38) {do_bcast = true, ginfo = #tpu.lg<out_addr = 7680, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521925920 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc86)
        %304 = "tpu.Load"(%40) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099515721680 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc87)
        %305 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7936, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099518049184 : i64>) -> tensor<1x64x1x9xi8> loc(#loc88)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 9, 19, 29, 39, 49, 59, 69], h_slice = [11, 12, 12, 12, 12, 12, 12, 11], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.084716969291338581>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.046328659842519687>> loc(#loc89)
        %307 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70], h_slice = [10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [80], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.046328659842519687>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>> loc(#loc84)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70], h_slice = [10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [80], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>, 0 : i64> loc(#loc84)
        "tpu.Yield"(%308) : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>, 0 : i64>) -> () loc(#loc84)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 8 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 0], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.084716969291338581>, 409600 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>, 0 : i64> loc(#loc84)
      %42 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517767552 : i64> loc(#loc90)
      %43 = "tpu.Group"(%41, %34) ({
        %302 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 65, 70, 75], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>, 0 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>> loc(#loc92)
        %303 = "tpu.Load"(%42) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517767552 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc93)
        %304 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 65, 70, 75], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 3276800 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>> loc(#loc94)
        %305 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 4096, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 65, 70, 75], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 3, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.040026512598425198>> loc(#loc95)
        %306 = "tpu.Add"(%304, %305) {do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 16384, buffer_size = 3840, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 65, 70, 75], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>, multipliers = [119, 71], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.040026512598425198>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>> loc(#loc91)
        %307 = "tpu.Store"(%306) {ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 65, 70, 75], h_slice = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 5, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>, 819200 : i64> loc(#loc91)
        "tpu.Yield"(%307) : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>, 819200 : i64>) -> () loc(#loc91)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 5, -2, 4, 0, 1], group_type = 0 : i64, hsecs = 14 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [0], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.065331917322834651>, 0 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 3276800 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>, 819200 : i64> loc(#loc91)
      %44 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099514010832 : i64> loc(#loc96)
      %45 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x64xsi8, 1099518012320 : i64> loc(#loc97)
      %46 = "tpu.Conv2D"(%43, %45, %44) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>, 819200 : i64>, tensor<1x64x9x64xsi8, 1099518012320 : i64>, tensor<1x64x1x9xi8, 1099514010832 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.061430120472440945>, 409600 : i64> loc(#loc98)
      %47 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518012064 : i64> loc(#loc99)
      %48 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099514756496 : i64> loc(#loc100)
      %49 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099517964768 : i64> loc(#loc101)
      %50 = "tpu.Group"(%46) ({
        %302 = "tpu.Load"(%46) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 9, 19, 29, 39, 49, 59, 69], h_slice = [11, 12, 12, 12, 12, 12, 12, 11], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.061430120472440945>, 409600 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.061430120472440945>> loc(#loc103)
        %303 = "tpu.Load"(%47) {do_bcast = true, ginfo = #tpu.lg<out_addr = 7680, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518012064 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc104)
        %304 = "tpu.Load"(%49) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099517964768 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc105)
        %305 = "tpu.Load"(%48) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7936, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099514756496 : i64>) -> tensor<1x64x1x9xi8> loc(#loc106)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 9, 19, 29, 39, 49, 59, 69], h_slice = [11, 12, 12, 12, 12, 12, 12, 11], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.061430120472440945>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.038892557480314963>> loc(#loc107)
        %307 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70], h_slice = [10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [80], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.038892557480314963>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>> loc(#loc102)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30, 40, 50, 60, 70], h_slice = [10, 10, 10, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [80], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>, 0 : i64> loc(#loc102)
        "tpu.Yield"(%308) : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>, 0 : i64>) -> () loc(#loc102)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 8 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 0], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.061430120472440945>, 409600 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>, 0 : i64> loc(#loc102)
      %51 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514755984 : i64> loc(#loc108)
      %52 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099522467696 : i64> loc(#loc109)
      %53 = "top.Weight"() : () -> tensor<1x128x1x256xsi8, 1099514767056 : i64> loc(#loc110)
      %54 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517920096 : i64> loc(#loc111)
      %55 = "tpu.Group"(%50, %43, %33, %34) ({
        %302 = "tpu.Load"(%50) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>, 0 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>> loc(#loc113)
        %303 = "tpu.Load"(%51) {do_bcast = true, ginfo = #tpu.lg<out_addr = 27776, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514755984 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc114)
        %304 = "tpu.Load"(%43) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>, 819200 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>> loc(#loc115)
        %305 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 8192, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 3, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071317785039370085>> loc(#loc116)
        %306 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>> loc(#loc117)
        %307 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 5, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 3276800 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>> loc(#loc94)
        %308 = "tpu.Add"(%304, %305) {do_relu = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3200, buffer_addr = 0, buffer_size = 3200, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 6, stage = 1, group_type = 0>, multipliers = [109, 109], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071317785039370085>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.08312323700787401>> loc(#loc118)
        %309 = "tpu.Load"(%53) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xsi8, 1099514767056 : i64>) -> tensor<1x128x1x256xsi8> loc(#loc119)
        %310 = "tpu.Load"(%52) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16000, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099522467696 : i64>) -> tensor<1x128x1x9xi8> loc(#loc120)
        %311 = "tpu.Concat"(%306, %307, %304, %308) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 9, stage = 1, group_type = 0>, multipliers = [64, 64, 69, 81], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 6, 6, 6]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.08312323700787401>>) -> tensor<1x256x80x80x!quant.uniform<i8:f32, 0.065660866141732277>> loc(#loc121)
        %312 = "tpu.Load"(%54) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31872, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 10, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517920096 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc122)
        %313 = "tpu.Conv2D"(%311, %309, %310) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 11, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80x!quant.uniform<i8:f32, 0.065660866141732277>>, tensor<1x128x1x256xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.075149491338582675>> loc(#loc123)
        %314 = "tpu.Lut"(%313, %312) {ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 12, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.075149491338582675>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>> loc(#loc112)
        %315 = "tpu.Store"(%314) {ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 13, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>, 2048000 : i64> loc(#loc112)
        "tpu.Yield"(%315) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>, 2048000 : i64>) -> () loc(#loc112)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 13, -2, 6, 4, 5, -3, 9, 7, 8, -4, 11, 10, -5, 12, 0, 1], group_type = 0 : i64, hsecs = 16 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [0], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077801366141732289>, 0 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071270800000000009>, 819200 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 2867200 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.066550658267716542>, 3276800 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>, 2048000 : i64> loc(#loc112)
      %56 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099514753680 : i64> loc(#loc124)
      %57 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x128xsi8, 1099517472640 : i64> loc(#loc125)
      %58 = "tpu.Conv2D"(%55, %57, %56) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>, 2048000 : i64>, tensor<1x256x9x128xsi8, 1099517472640 : i64>, tensor<1x256x1x9xi8, 1099514753680 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065766281889763772>, 409600 : i64> loc(#loc126)
      %59 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514163344 : i64> loc(#loc127)
      %60 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099518050272 : i64> loc(#loc128)
      %61 = "top.Weight"() : () -> tensor<1x256x1x256xsi8, 1099522468848 : i64> loc(#loc129)
      %62 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521926688 : i64> loc(#loc130)
      %63 = "tpu.Group"(%58) ({
        %302 = "tpu.Load"(%58) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 38], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065766281889763772>, 409600 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065766281889763772>> loc(#loc132)
        %303 = "tpu.Load"(%59) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24864, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514163344 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc133)
        %304 = "tpu.Load"(%61) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xsi8, 1099522468848 : i64>) -> tensor<1x256x1x256xsi8> loc(#loc134)
        %305 = "tpu.Load"(%60) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099518050272 : i64>) -> tensor<1x256x1x9xi8> loc(#loc135)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 38], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065766281889763772>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.045889892125984252>> loc(#loc136)
        %307 = "tpu.Load"(%62) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521926688 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc137)
        %308 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 38], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2], w_idx = [0], w_slice = [40], id = 6, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.045889892125984252>>, tensor<1x256x1x256xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068604169291338585>> loc(#loc138)
        %309 = "tpu.Lut"(%308, %307) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 38], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068604169291338585>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>> loc(#loc131)
        %310 = "tpu.Store"(%309) {ginfo = #tpu.lg<out_addr = 20480, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 38], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2], w_idx = [0], w_slice = [40], id = 8, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64> loc(#loc131)
        "tpu.Yield"(%310) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64>) -> () loc(#loc131)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 8, -2, 6, 5, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 14 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065766281889763772>, 409600 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64> loc(#loc131)
      %64 = "tpu.Slice"(%63, %0, %0, %0, %0) {axes = [], ends = [-1, 128, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64>, none, none, none, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64> loc(#loc139)
      %65 = "tpu.Slice"(%63, %0, %0, %0, %0) {axes = [], ends = [-1, 256, -1, -1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64>, none, none, none, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 204800 : i64> loc(#loc140)
      %66 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099514159376 : i64> loc(#loc141)
      %67 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099514011920 : i64> loc(#loc142)
      %68 = "tpu.Conv2D"(%65, %67, %66) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 204800 : i64>, tensor<1x128x9x128xsi8, 1099514011920 : i64>, tensor<1x128x1x9xi8, 1099514159376 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.060954687401574802>, 614400 : i64> loc(#loc143)
      %69 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521925664 : i64> loc(#loc144)
      %70 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099521567008 : i64> loc(#loc145)
      %71 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099516282832 : i64> loc(#loc146)
      %72 = "tpu.Group"(%68) ({
        %302 = "tpu.Load"(%68) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.060954687401574802>, 614400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.060954687401574802>> loc(#loc148)
        %303 = "tpu.Load"(%69) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521925664 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc149)
        %304 = "tpu.Load"(%71) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099516282832 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc150)
        %305 = "tpu.Load"(%70) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099521567008 : i64>) -> tensor<1x128x1x9xi8> loc(#loc151)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.060954687401574802>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.053819701574803154>> loc(#loc152)
        %307 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.053819701574803154>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>> loc(#loc147)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>, 409600 : i64> loc(#loc147)
        "tpu.Yield"(%308) : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>, 409600 : i64>) -> () loc(#loc147)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 10 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [6], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.060954687401574802>, 614400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>, 409600 : i64> loc(#loc147)
      %73 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099513584592 : i64> loc(#loc153)
      %74 = "tpu.Group"(%72, %65) ({
        %302 = "tpu.Load"(%72) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 35], h_slice = [6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>, 409600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>> loc(#loc155)
        %303 = "tpu.Load"(%73) {do_bcast = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099513584592 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc156)
        %304 = "tpu.Load"(%65) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 35], h_slice = [6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 204800 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>> loc(#loc157)
        %305 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 0, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 35], h_slice = [6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [40], id = 3, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.055568004724409446>> loc(#loc158)
        %306 = "tpu.Add"(%304, %305) {do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3840, buffer_addr = 16384, buffer_size = 3840, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 35], h_slice = [6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>, multipliers = [116, 115], relu_limit = -1.000000e+00 : f64, rshifts = [7]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.055568004724409446>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>> loc(#loc154)
        %307 = "tpu.Store"(%306) {ginfo = #tpu.lg<out_addr = 8192, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 6, 12, 18, 24, 30, 35], h_slice = [6, 6, 6, 6, 6, 5, 5], w_idx = [0], w_slice = [40], id = 5, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>, 819200 : i64> loc(#loc154)
        "tpu.Yield"(%307) : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>, 819200 : i64>) -> () loc(#loc154)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 5, -2, 4, 0, 1], group_type = 0 : i64, hsecs = 7 : i64, nsecs = 1 : i64, other_down_overlap_op = [-1, 6], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.07626308661417322>, 409600 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 204800 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>, 819200 : i64> loc(#loc154)
      %75 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099522573232 : i64> loc(#loc159)
      %76 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099515574224 : i64> loc(#loc160)
      %77 = "tpu.Conv2D"(%74, %76, %75) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>, 819200 : i64>, tensor<1x128x9x128xsi8, 1099515574224 : i64>, tensor<1x128x1x9xi8, 1099522573232 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071009462992125988>, 614400 : i64> loc(#loc161)
      %78 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512973264 : i64> loc(#loc162)
      %79 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099516434384 : i64> loc(#loc163)
      %80 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099514800080 : i64> loc(#loc164)
      %81 = "tpu.Group"(%77) ({
        %302 = "tpu.Load"(%77) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071009462992125988>, 614400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071009462992125988>> loc(#loc166)
        %303 = "tpu.Load"(%78) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512973264 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc167)
        %304 = "tpu.Load"(%80) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099514800080 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc168)
        %305 = "tpu.Load"(%79) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099516434384 : i64>) -> tensor<1x128x1x9xi8> loc(#loc169)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071009462992125988>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.044721665354330711>> loc(#loc170)
        %307 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.044721665354330711>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>> loc(#loc165)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>, 409600 : i64> loc(#loc165)
        "tpu.Yield"(%308) : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>, 409600 : i64>) -> () loc(#loc165)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 10 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071009462992125988>, 614400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>, 409600 : i64> loc(#loc165)
      %82 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517471696 : i64> loc(#loc171)
      %83 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099521873952 : i64> loc(#loc172)
      %84 = "top.Weight"() : () -> tensor<1x256x1x512xsi8, 1099513879760 : i64> loc(#loc173)
      %85 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518053456 : i64> loc(#loc174)
      %86 = "tpu.Group"(%81, %74, %64, %65) ({
        %302 = "tpu.Load"(%81) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>, 409600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>> loc(#loc176)
        %303 = "tpu.Load"(%82) {do_bcast = true, ginfo = #tpu.lg<out_addr = 23584, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517471696 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc177)
        %304 = "tpu.Load"(%74) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>, 819200 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>> loc(#loc178)
        %305 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 3, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.08879540708661418>> loc(#loc179)
        %306 = "tpu.Load"(%64) {do_bcast = false, ginfo = #tpu.lg<out_addr = 21760, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>> loc(#loc180)
        %307 = "tpu.Load"(%65) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25856, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 204800 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>> loc(#loc157)
        %308 = "tpu.Load"(%83) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23040, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099521873952 : i64>) -> tensor<1x256x1x9xi8> loc(#loc181)
        %309 = "tpu.Add"(%304, %305) {do_relu = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 16384, buffer_size = 1280, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>, multipliers = [45, 65], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.08879540708661418>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.087104112598425196>> loc(#loc182)
        %310 = "tpu.Load"(%84) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xsi8, 1099513879760 : i64>) -> tensor<1x256x1x512xsi8> loc(#loc183)
        %311 = "tpu.Concat"(%306, %307, %304, %309) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 9, stage = 1, group_type = 0>, multipliers = [79, 79, 87, 123], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 7, 7]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.087104112598425196>>) -> tensor<1x512x40x40x!quant.uniform<i8:f32, 0.090386350393700785>> loc(#loc184)
        %312 = "tpu.Load"(%85) {do_bcast = true, ginfo = #tpu.lg<out_addr = 23328, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 10, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518053456 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc185)
        %313 = "tpu.Conv2D"(%311, %310, %308) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 11, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40x!quant.uniform<i8:f32, 0.090386350393700785>>, tensor<1x256x1x512xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.071502497637795273>> loc(#loc186)
        %314 = "tpu.Lut"(%313, %312) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 12, stage = 1, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.071502497637795273>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>> loc(#loc175)
        %315 = "tpu.Store"(%314) {ginfo = #tpu.lg<out_addr = 16384, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 13, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>, 2867200 : i64> loc(#loc175)
        "tpu.Yield"(%315) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>, 2867200 : i64>) -> () loc(#loc175)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 13, -2, 7, 4, 5, 6, -3, 9, 8, -4, 11, 10, -5, 12, 0, 1], group_type = 0 : i64, hsecs = 20 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.10128279133858269>, 409600 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.061820126771653543>, 819200 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 0 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.056326437795275595>, 204800 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>, 2867200 : i64> loc(#loc175)
      %87 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099521868320 : i64> loc(#loc187)
      %88 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x9x256xsi8, 1099518053968 : i64> loc(#loc188)
      %89 = "tpu.Conv2D"(%86, %88, %87) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>, 2867200 : i64>, tensor<1x512x9x256xsi8, 1099518053968 : i64>, tensor<1x512x1x9xi8, 1099521868320 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.064728999999999995>, 0 : i64> loc(#loc189)
      %90 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521926176 : i64> loc(#loc190)
      %91 = "tpu.Lut"(%89, %90) : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.064728999999999995>, 0 : i64>, tensor<1x1x1x256xsi8, 1099521926176 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.053923037795275588>, 204800 : i64> loc(#loc191)
      %92 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099519747792 : i64> loc(#loc192)
      %93 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x512xsi8, 1099519381904 : i64> loc(#loc193)
      %94 = "tpu.Conv2D"(%91, %93, %92) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.053923037795275588>, 204800 : i64>, tensor<1x512x1x512xsi8, 1099519381904 : i64>, tensor<1x512x1x9xi8, 1099519747792 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.083325618897637788>, 0 : i64> loc(#loc194)
      %95 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518006304 : i64> loc(#loc195)
      %96 = "tpu.Lut"(%94, %95) : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.083325618897637788>, 0 : i64>, tensor<1x1x1x256xsi8, 1099518006304 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 307200 : i64> loc(#loc196)
      %97 = "tpu.Slice"(%96, %0, %0, %0, %0) {axes = [], ends = [-1, 256, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 307200 : i64>, none, none, none, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 307200 : i64> loc(#loc197)
      %98 = "tpu.Slice"(%96, %0, %0, %0, %0) {axes = [], ends = [-1, 512, -1, -1], offset = [0, 256, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 307200 : i64>, none, none, none, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 409600 : i64> loc(#loc198)
      %99 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099519743184 : i64> loc(#loc199)
      %100 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099512994768 : i64> loc(#loc200)
      %101 = "tpu.Conv2D"(%98, %100, %99) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 409600 : i64>, tensor<1x256x9x256xsi8, 1099512994768 : i64>, tensor<1x256x1x9xi8, 1099519743184 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.082138704724409442>, 102400 : i64> loc(#loc201)
      %102 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517920352 : i64> loc(#loc202)
      %103 = "tpu.Lut"(%101, %102) : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.082138704724409442>, 102400 : i64>, tensor<1x1x1x256xsi8, 1099517920352 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.088517822047244093>, 0 : i64> loc(#loc203)
      %104 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099519745488 : i64> loc(#loc204)
      %105 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099519752656 : i64> loc(#loc205)
      %106 = "tpu.Conv2D"(%103, %105, %104) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.088517822047244093>, 0 : i64>, tensor<1x256x9x256xsi8, 1099519752656 : i64>, tensor<1x256x1x9xi8, 1099519745488 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12362240629921259>, 512000 : i64> loc(#loc206)
      %107 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512974672 : i64> loc(#loc207)
      %108 = "tpu.Group"(%106, %98, %97) ({
        %302 = "tpu.Load"(%106) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12362240629921259>, 512000 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12362240629921259>> loc(#loc209)
        %303 = "tpu.Load"(%107) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10752, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512974672 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc210)
        %304 = "tpu.Load"(%98) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 409600 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>> loc(#loc211)
        %305 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 3, stage = 1, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12362240629921259>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12780522283464565>> loc(#loc212)
        %306 = "tpu.Load"(%97) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 4, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 307200 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>> loc(#loc213)
        %307 = "tpu.Add"(%304, %305) {do_relu = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3584, buffer_addr = 0, buffer_size = 3584, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 5, stage = 1, group_type = 0>, multipliers = [46, 86], relu_limit = -1.000000e+00 : f64, rshifts = [6]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12780522283464565>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.094787297637795281>> loc(#loc214)
        %308 = "tpu.Concat"(%306, %304, %307) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10752, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 6, stage = 1, group_type = 0>, multipliers = [67, 67, 94], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 7]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.094787297637795281>>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.12885147716535433>> loc(#loc208)
        %309 = "tpu.Store"(%308) {ginfo = #tpu.lg<out_addr = 0, out_size = 10752, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 7, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.12885147716535433>>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.12885147716535433>, 0 : i64> loc(#loc208)
        "tpu.Yield"(%309) : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.12885147716535433>, 0 : i64>) -> () loc(#loc208)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 7, -2, 5, 4, -3, 6, 0, 1], group_type = 0 : i64, hsecs = 4 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12362240629921259>, 512000 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 409600 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068145899212598435>, 307200 : i64>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.12885147716535433>, 0 : i64> loc(#loc208)
      %109 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099520342480 : i64> loc(#loc215)
      %110 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x768xsi8, 1099520347088 : i64> loc(#loc216)
      %111 = "tpu.Conv2D"(%108, %110, %109) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.12885147716535433>, 0 : i64>, tensor<1x512x1x768xsi8, 1099520347088 : i64>, tensor<1x512x1x9xi8, 1099520342480 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.091494522834645667>, 409600 : i64> loc(#loc217)
      %112 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519752400 : i64> loc(#loc218)
      %113 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099517769248 : i64> loc(#loc219)
      %114 = "top.Weight"() : () -> tensor<1x256x1x512xsi8, 1099520740304 : i64> loc(#loc220)
      %115 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099520945104 : i64> loc(#loc221)
      %116 = "tpu.Group"(%111) ({
        %302 = "tpu.Load"(%111) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.091494522834645667>, 409600 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.091494522834645667>> loc(#loc223)
        %303 = "tpu.Load"(%112) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32512, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519752400 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc224)
        %304 = "tpu.Load"(%114) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xsi8, 1099520740304 : i64>) -> tensor<1x256x1x512xsi8> loc(#loc225)
        %305 = "tpu.Load"(%113) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24064, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099517769248 : i64>) -> tensor<1x256x1x9xi8> loc(#loc226)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 4, stage = 1, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.091494522834645667>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087303185826771657>> loc(#loc227)
        %307 = "tpu.Load"(%115) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32256, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099520945104 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc228)
        %308 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 29696, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 6, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087303185826771657>>, tensor<1x256x1x512xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.084541425984251975>> loc(#loc229)
        %309 = "tpu.Lut"(%308, %307) {ginfo = #tpu.lg<out_addr = 21504, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 7, stage = 1, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.084541425984251975>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>> loc(#loc222)
        %310 = "tpu.Store"(%309) {ginfo = #tpu.lg<out_addr = 21504, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 8, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64> loc(#loc222)
        "tpu.Yield"(%310) : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64>) -> () loc(#loc222)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, -2, 6, 5, 8, -3, 7, 0, 1], group_type = 0 : i64, hsecs = 5 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.091494522834645667>, 409600 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64> loc(#loc222)
      %117 = "tpu.Pool2D"(%116) {count_include_pad = false, do_relu = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, strides = [1, 1]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 102400 : i64> loc(#loc230)
      %118 = "tpu.Pool2D"(%117) {count_include_pad = false, do_relu = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, strides = [1, 1]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 102400 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 204800 : i64> loc(#loc231)
      %119 = "tpu.Pool2D"(%118) {count_include_pad = false, do_relu = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, strides = [1, 1]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 204800 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 307200 : i64> loc(#loc232)
      %120 = "tpu.Concat"(%116, %117, %118, %119) {axis = 1 : si32, do_relu = false, multipliers = [1, 1, 1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 0, 0]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 102400 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 204800 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 307200 : i64>) -> tensor<1x1024x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64> loc(#loc233)
      %121 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099520945360 : i64> loc(#loc234)
      %122 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x1024xsi8, 1099515758544 : i64> loc(#loc235)
      %123 = "tpu.Conv2D"(%120, %122, %121) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x1024x20x20x!quant.uniform<i8:f32, 0.1126667905511811>, 0 : i64>, tensor<1x512x1x1024xsi8, 1099515758544 : i64>, tensor<1x512x1x9xi8, 1099520945360 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.051372993700787399>, 3276800 : i64> loc(#loc236)
      %124 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521343200 : i64> loc(#loc237)
      %125 = "top.Weight"() : () -> tensor<1x512x2x2xsi8, 1099521361824 : i64> loc(#loc238)
      %126 = "top.Weight"() : () -> tensor<1x512x1x5xi8, 1099521363872 : i64> loc(#loc239)
      %127:2 = "tpu.Group"(%123) ({
        %302 = "tpu.Load"(%123) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.051372993700787399>, 3276800 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.051372993700787399>> loc(#loc242)
        %303 = "tpu.Load"(%124) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32064, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521343200 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc243)
        %304 = "tpu.Load"(%125) {do_bcast = false, ginfo = #tpu.lg<out_addr = 30720, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x2x2xsi8, 1099521361824 : i64>) -> tensor<1x512x2x2xsi8> loc(#loc244)
        %305 = "tpu.Load"(%126) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31744, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x5xi8, 1099521363872 : i64>) -> tensor<1x512x1x5xi8> loc(#loc245)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 4, stage = 1, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.051372993700787399>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>> loc(#loc240)
        %307 = "tpu.Store"(%306) {ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 5, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>, 3686400 : i64> loc(#loc240)
        %308 = "tpu.Deconv"(%306, %304, %305) {dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 20480, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 6, stage = 1, group_type = 0>, group = 512 : i64, inserts = [0, 0], kernel_shape = [2, 2], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], with_bias = false} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>>, tensor<1x512x2x2xsi8>, tensor<1x512x1x5xi8>) -> tensor<1x512x40x40x!quant.uniform<i8:f32, 0.038480098425196847>> loc(#loc241)
        %309 = "tpu.Store"(%308) {ginfo = #tpu.lg<out_addr = 0, out_size = 20480, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 7, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x512x40x40x!quant.uniform<i8:f32, 0.038480098425196847>>) -> tensor<1x512x40x40x!quant.uniform<i8:f32, 0.038480098425196847>, 1228800 : i64> loc(#loc241)
        "tpu.Yield"(%307, %309) : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>, 3686400 : i64>, tensor<1x512x40x40x!quant.uniform<i8:f32, 0.038480098425196847>, 1228800 : i64>) -> () loc(#loc564)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 7, -2, 6, 5, 0, 1], group_type = 0 : i64, hsecs = 5 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.051372993700787399>, 3276800 : i64>) -> (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>, 3686400 : i64>, tensor<1x512x40x40x!quant.uniform<i8:f32, 0.038480098425196847>, 1228800 : i64>) loc(#loc564)
      %128 = "tpu.Concat"(%127#1, %86) {axis = 1 : si32, do_relu = false, multipliers = [108, 96], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 6]} : (tensor<1x512x40x40x!quant.uniform<i8:f32, 0.038480098425196847>, 1228800 : i64>, tensor<1x256x40x40x!quant.uniform<i8:f32, 0.068194662992125982>, 2867200 : i64>) -> tensor<1x768x40x40x!quant.uniform<i8:f32, 0.045266043307086612>, 0 : i64> loc(#loc246)
      %129 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099521367840 : i64> loc(#loc247)
      %130 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x768xsi8, 1099521370144 : i64> loc(#loc248)
      %131 = "tpu.Conv2D"(%128, %130, %129) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x40x40x!quant.uniform<i8:f32, 0.045266043307086612>, 0 : i64>, tensor<1x256x1x768xsi8, 1099521370144 : i64>, tensor<1x256x1x9xi8, 1099521367840 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.082929536220472444>, 1228800 : i64> loc(#loc249)
      %132 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521568160 : i64> loc(#loc250)
      %133 = "tpu.Lut"(%131, %132) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.082929536220472444>, 1228800 : i64>, tensor<1x1x1x256xsi8, 1099521568160 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 0 : i64> loc(#loc251)
      %134 = "tpu.Slice"(%133, %0, %0, %0, %0) {axes = [], ends = [-1, 128, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 0 : i64>, none, none, none, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 0 : i64> loc(#loc252)
      %135 = "tpu.Slice"(%133, %0, %0, %0, %0) {axes = [], ends = [-1, 256, -1, -1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 0 : i64>, none, none, none, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 204800 : i64> loc(#loc253)
      %136 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099521359840 : i64> loc(#loc254)
      %137 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099521568416 : i64> loc(#loc255)
      %138 = "tpu.Conv2D"(%135, %137, %136) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 204800 : i64>, tensor<1x128x9x128xsi8, 1099521568416 : i64>, tensor<1x128x1x9xi8, 1099521359840 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071789809448818895>, 409600 : i64> loc(#loc256)
      %139 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518050016 : i64> loc(#loc257)
      %140 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099521715872 : i64> loc(#loc258)
      %141 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099521718304 : i64> loc(#loc259)
      %142 = "tpu.Group"(%138) ({
        %302 = "tpu.Load"(%138) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071789809448818895>, 409600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071789809448818895>> loc(#loc261)
        %303 = "tpu.Load"(%139) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518050016 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc262)
        %304 = "tpu.Load"(%141) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099521718304 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc263)
        %305 = "tpu.Load"(%140) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099521715872 : i64>) -> tensor<1x128x1x9xi8> loc(#loc264)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071789809448818895>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052923676377952755>> loc(#loc265)
        %307 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052923676377952755>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>> loc(#loc260)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>, 2867200 : i64> loc(#loc260)
        "tpu.Yield"(%308) : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>, 2867200 : i64>) -> () loc(#loc260)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 10 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 1], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.071789809448818895>, 409600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>, 2867200 : i64> loc(#loc260)
      %143 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519233616 : i64> loc(#loc266)
      %144 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099521865760 : i64> loc(#loc267)
      %145 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099519644048 : i64> loc(#loc268)
      %146 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514163088 : i64> loc(#loc269)
      %147 = "tpu.Group"(%142, %134, %135) ({
        %302 = "tpu.Load"(%142) {do_bcast = false, ginfo = #tpu.lg<out_addr = 17408, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>, 2867200 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>> loc(#loc271)
        %303 = "tpu.Load"(%143) {do_bcast = true, ginfo = #tpu.lg<out_addr = 27136, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519233616 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc272)
        %304 = "tpu.Load"(%134) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 0 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>> loc(#loc273)
        %305 = "tpu.Load"(%135) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 204800 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>> loc(#loc274)
        %306 = "tpu.Load"(%144) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099521865760 : i64>) -> tensor<1x256x1x9xi8> loc(#loc275)
        %307 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066642322047244087>> loc(#loc276)
        %308 = "tpu.Load"(%145) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099519644048 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc277)
        %309 = "tpu.Concat"(%304, %305, %307) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>, multipliers = [126, 126, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 0]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066642322047244087>>) -> tensor<1x384x40x40x!quant.uniform<i8:f32, 0.066642322047244087>> loc(#loc278)
        %310 = "tpu.Load"(%146) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31232, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514163088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc279)
        %311 = "tpu.Conv2D"(%309, %308, %306) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 9, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x40x40x!quant.uniform<i8:f32, 0.066642322047244087>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.11461695275590551>> loc(#loc280)
        %312 = "tpu.Lut"(%311, %310) {ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 10, stage = 1, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.11461695275590551>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>> loc(#loc270)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64> loc(#loc270)
        "tpu.Yield"(%313) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64>) -> () loc(#loc270)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 11, 4, -2, 7, 6, -3, 9, 8, -4, 10, 0, 1], group_type = 0 : i64, hsecs = 10 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-4, 1, 2], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.073047566141732273>, 2867200 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 0 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.065625501574803152>, 204800 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64> loc(#loc270)
      %148 = "top.Weight"() : () -> tensor<1x256x2x2xsi8, 1099521872928 : i64> loc(#loc281)
      %149 = "top.Weight"() : () -> tensor<1x256x1x5xi8, 1099521717024 : i64> loc(#loc282)
      %150 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099521366432 : i64> loc(#loc283)
      %151 = "top.Weight"() : () -> tensor<1x128x1x384xsi8, 1099521876256 : i64> loc(#loc284)
      %152 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521926432 : i64> loc(#loc285)
      %153 = "tpu.Group"(%147, %55) ({
        %302 = "tpu.Load"(%147) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>> loc(#loc287)
        %303 = "tpu.Load"(%148) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19712, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [2], w_idx = [0], w_slice = [2], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x2x2xsi8, 1099521872928 : i64>) -> tensor<1x256x2x2xsi8> loc(#loc288)
        %304 = "tpu.Load"(%149) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20224, out_size = 160, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [5], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x5xi8, 1099521717024 : i64>) -> tensor<1x256x1x5xi8> loc(#loc289)
        %305 = "tpu.Load"(%55) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>, 2048000 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>> loc(#loc290)
        %306 = "tpu.Deconv"(%302, %303, %304) {dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>, group = 256 : i64, inserts = [0, 0], kernel_shape = [2, 2], pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], with_bias = false} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>>, tensor<1x256x2x2xsi8>, tensor<1x256x1x5xi8>) -> tensor<1x256x80x80x!quant.uniform<i8:f32, 0.064309346456692909>> loc(#loc291)
        %307 = "tpu.Load"(%151) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x384xsi8, 1099521876256 : i64>) -> tensor<1x128x1x384xsi8> loc(#loc292)
        %308 = "tpu.Load"(%150) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28416, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099521366432 : i64>) -> tensor<1x128x1x9xi8> loc(#loc293)
        %309 = "tpu.Concat"(%306, %305) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 11520, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 7, stage = 1, group_type = 0>, multipliers = [1, 109], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 7]} : (tensor<1x256x80x80x!quant.uniform<i8:f32, 0.064309346456692909>>, tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>>) -> tensor<1x384x80x80x!quant.uniform<i8:f32, 0.064309346456692909>> loc(#loc294)
        %310 = "tpu.Load"(%152) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28160, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521926432 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc295)
        %311 = "tpu.Conv2D"(%309, %307, %308) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 9, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x80x80x!quant.uniform<i8:f32, 0.064309346456692909>>, tensor<1x128x1x384xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.079877237795275585>> loc(#loc296)
        %312 = "tpu.Lut"(%311, %310) {ginfo = #tpu.lg<out_addr = 6144, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 10, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.079877237795275585>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>> loc(#loc286)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 6144, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64> loc(#loc286)
        "tpu.Yield"(%313) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64>) -> () loc(#loc286)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 11, -2, 7, 5, 6, -3, 9, 8, -4, 10, 0, 1, 2], group_type = 0 : i64, hsecs = 27 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [2, 1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64>, tensor<1x128x80x80x!quant.uniform<i8:f32, 0.05504761181102362>, 2048000 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64> loc(#loc286)
      %154 = "tpu.Slice"(%153, %0, %0, %0, %0) {axes = [], ends = [-1, 64, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64>, none, none, none, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64> loc(#loc297)
      %155 = "tpu.Slice"(%153, %0, %0, %0, %0) {axes = [], ends = [-1, 128, -1, -1], offset = [0, 64, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64>, none, none, none, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 409600 : i64> loc(#loc298)
      %156 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099519233872 : i64> loc(#loc299)
      %157 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x64xsi8, 1099522574384 : i64> loc(#loc300)
      %158 = "tpu.Conv2D"(%155, %157, %156) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 409600 : i64>, tensor<1x64x9x64xsi8, 1099522574384 : i64>, tensor<1x64x1x9xi8, 1099519233872 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.056182819685039374>, 1638400 : i64> loc(#loc301)
      %159 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514766800 : i64> loc(#loc302)
      %160 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099521926944 : i64> loc(#loc303)
      %161 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099521927520 : i64> loc(#loc304)
      %162 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521964384 : i64> loc(#loc305)
      %163 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099522367072 : i64> loc(#loc306)
      %164 = "top.Weight"() : () -> tensor<1x128x1x192xsi8, 1099522368224 : i64> loc(#loc307)
      %165 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521925408 : i64> loc(#loc308)
      %166 = "tpu.Group"(%158, %154, %155) ({
        %302 = "tpu.Load"(%158) {do_bcast = false, ginfo = #tpu.lg<out_addr = 27648, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74], h_slice = [6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.056182819685039374>, 1638400 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.056182819685039374>> loc(#loc310)
        %303 = "tpu.Load"(%159) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24016, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514766800 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc311)
        %304 = "tpu.Load"(%161) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099521927520 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc312)
        %305 = "tpu.Load"(%160) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23680, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099521926944 : i64>) -> tensor<1x64x1x9xi8> loc(#loc313)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 12288, out_size = 4480, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74], h_slice = [6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.056182819685039374>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.0454001094488189>> loc(#loc314)
        %307 = "tpu.Load"(%162) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24272, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521964384 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc315)
        %308 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 6, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.0454001094488189>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077917789763779526>> loc(#loc316)
        %309 = "tpu.Load"(%154) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 7, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>> loc(#loc317)
        %310 = "tpu.Load"(%155) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 8, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 409600 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>> loc(#loc318)
        %311 = "tpu.Lut"(%308, %307) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 9, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.077917789763779526>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.064906008661417325>> loc(#loc319)
        %312 = "tpu.Load"(%164) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 10, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x192xsi8, 1099522368224 : i64>) -> tensor<1x128x1x192xsi8> loc(#loc320)
        %313 = "tpu.Load"(%163) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32128, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 11, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099522367072 : i64>) -> tensor<1x128x1x9xi8> loc(#loc321)
        %314 = "tpu.Concat"(%309, %310, %311) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 9600, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [192], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 12, stage = 1, group_type = 0>, multipliers = [63, 63, 120], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 6, 7]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.064906008661417325>>) -> tensor<1x192x80x80x!quant.uniform<i8:f32, 0.068799355118110236>> loc(#loc322)
        %315 = "tpu.Load"(%165) {do_bcast = true, ginfo = #tpu.lg<out_addr = 23760, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 13, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521925408 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc323)
        %316 = "tpu.Conv2D"(%314, %312, %313) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 14, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x192x80x80x!quant.uniform<i8:f32, 0.068799355118110236>>, tensor<1x128x1x192xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.076622888188976376>> loc(#loc324)
        %317 = "tpu.Lut"(%316, %315) {ginfo = #tpu.lg<out_addr = 4608, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 15, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.076622888188976376>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>> loc(#loc309)
        %318 = "tpu.Store"(%317) {ginfo = #tpu.lg<out_addr = 4608, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], h_slice = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], w_idx = [0], w_slice = [80], id = 16, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64> loc(#loc309)
        "tpu.Yield"(%318) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64>) -> () loc(#loc309)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 16, -2, 6, 5, -3, 9, 7, 8, -4, 12, 10, 11, -5, 14, 13, -6, 15, 0, 1], group_type = 0 : i64, hsecs = 16 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.056182819685039374>, 1638400 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 0 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.068788246456692917>, 409600 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64> loc(#loc309)
      %167 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099517957472 : i64> loc(#loc325)
      %168 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099516435536 : i64> loc(#loc326)
      %169 = "tpu.Conv2D"(%166, %168, %167) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64>, tensor<1x128x9x128xsi8, 1099516435536 : i64>, tensor<1x128x1x9xi8, 1099517957472 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.1328436598425197>, 2662400 : i64> loc(#loc327)
      %170 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099519742352 : i64> loc(#loc328)
      %171 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x128xsi8, 1099522392800 : i64> loc(#loc329)
      %172 = "tpu.Conv2D"(%166, %171, %170) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64>, tensor<1x64x9x128xsi8, 1099522392800 : i64>, tensor<1x64x1x9xi8, 1099519742352 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.096457053543307095>, 2252800 : i64> loc(#loc330)
      %173 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099522466528 : i64> loc(#loc331)
      %174 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099517772128 : i64> loc(#loc332)
      %175 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521868064 : i64> loc(#loc333)
      %176:2 = "tpu.Group"(%166, %169) ({
        %302 = "tpu.Load"(%166) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77], h_slice = [4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>> loc(#loc336)
        %303 = "tpu.Load"(%174) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099517772128 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc337)
        %304 = "tpu.Load"(%173) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099522466528 : i64>) -> tensor<1x128x1x9xi8> loc(#loc338)
        %305 = "tpu.Load"(%169) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], w_idx = [0], w_slice = [40], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.1328436598425197>, 2662400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.1328436598425197>> loc(#loc339)
        %306 = "tpu.Load"(%175) {do_bcast = true, ginfo = #tpu.lg<out_addr = 19712, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521868064 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc340)
        %307 = "tpu.Conv2D"(%302, %303, %304) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>> loc(#loc334)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 28672, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78], h_slice = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2], w_idx = [0], w_slice = [80], id = 6, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>, 0 : i64> loc(#loc334)
        %309 = "tpu.Lut"(%305, %306) {ginfo = #tpu.lg<out_addr = 26880, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.1328436598425197>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>> loc(#loc335)
        %310 = "tpu.Store"(%309) {ginfo = #tpu.lg<out_addr = 26880, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], w_idx = [0], w_slice = [40], id = 8, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>, 3481600 : i64> loc(#loc335)
        "tpu.Yield"(%308, %310) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>, 0 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>, 3481600 : i64>) -> () loc(#loc565)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 8, -2, 7, 6, 0, 1, 2], group_type = 0 : i64, hsecs = 27 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.068568390551181108>, 819200 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.1328436598425197>, 2662400 : i64>) -> (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>, 0 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>, 3481600 : i64>) loc(#loc565)
      %177 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518053712 : i64> loc(#loc341)
      %178 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099522534384 : i64> loc(#loc342)
      %179 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099522534640 : i64> loc(#loc343)
      %180 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099515537360 : i64> loc(#loc344)
      %181:3 = "tpu.Group"(%172, %176#0, %176#1, %147) ({
        %302 = "tpu.Load"(%172) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [80], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.096457053543307095>, 2252800 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.096457053543307095>> loc(#loc348)
        %303 = "tpu.Load"(%177) {do_bcast = true, ginfo = #tpu.lg<out_addr = 27136, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518053712 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc349)
        %304 = "tpu.Load"(%176#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4608, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>, 0 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>> loc(#loc350)
        %305 = "tpu.Load"(%178) {do_bcast = true, ginfo = #tpu.lg<out_addr = 12032, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099522534384 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc351)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 12288, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [80], id = 4, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.096457053543307095>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.088387562204724406>> loc(#loc352)
        %307 = "tpu.Load"(%176#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>, 3481600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>> loc(#loc353)
        %308 = "tpu.Load"(%147) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 6, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>> loc(#loc287)
        %309 = "tpu.Lut"(%304, %305) {ginfo = #tpu.lg<out_addr = 16128, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 7, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13108384173228346>> loc(#loc345)
        %310 = "tpu.Load"(%180) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099515537360 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc354)
        %311 = "tpu.Load"(%179) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23040, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099522534640 : i64>) -> tensor<1x64x1x9xi8> loc(#loc355)
        %312 = "tpu.Store"(%309) {ginfo = #tpu.lg<out_addr = 16128, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 10, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13108384173228346>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13108384173228346>, 819200 : i64> loc(#loc345)
        %313 = "tpu.Concat"(%307, %308) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 11, stage = 1, group_type = 0>, multipliers = [90, 84], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>>, tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>>) -> tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>> loc(#loc346)
        %314 = "tpu.Store"(%313) {ginfo = #tpu.lg<out_addr = 8192, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 12, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>>) -> tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>, 1638400 : i64> loc(#loc346)
        %315 = "tpu.Conv2D"(%306, %310, %311) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 13, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.088387562204724406>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>> loc(#loc347)
        %316 = "tpu.Store"(%315) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 14, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>, 2662400 : i64> loc(#loc347)
        "tpu.Yield"(%312, %314, %316) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13108384173228346>, 819200 : i64>, tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>, 1638400 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>, 2662400 : i64>) -> () loc(#loc566)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 14, -2, 7, 5, 6, -3, 11, 8, 9, 10, -4, 13, 12, 0, 1], group_type = 0 : i64, hsecs = 20 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.096457053543307095>, 2252800 : i64>, tensor<1x128x80x80x!quant.uniform<i8:f32, 0.11812448818897639>, 0 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.091856577165354322>, 3481600 : i64>, tensor<1x256x40x40x!quant.uniform<i8:f32, 0.0860876220472441>, 3072000 : i64>) -> (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13108384173228346>, 819200 : i64>, tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>, 1638400 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>, 2662400 : i64>) loc(#loc566)
      %182 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099522572080 : i64> loc(#loc356)
      %183 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099522611248 : i64> loc(#loc357)
      %184 = "tpu.Conv2D"(%181#0, %183, %182) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13108384173228346>, 819200 : i64>, tensor<1x128x9x128xsi8, 1099522611248 : i64>, tensor<1x128x1x9xi8, 1099522572080 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.23008161811023622>, 0 : i64> loc(#loc358)
      %185 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099522758704 : i64> loc(#loc359)
      %186 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099522761264 : i64> loc(#loc360)
      %187 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099516881616 : i64> loc(#loc361)
      %188 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099519742928 : i64> loc(#loc362)
      %189 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099522761008 : i64> loc(#loc363)
      %190 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512972432 : i64> loc(#loc364)
      %191 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099518002208 : i64> loc(#loc365)
      %192 = "top.Weight"() : () -> tensor<1x1x1x9xi8, 1099522467680 : i64> loc(#loc366)
      %193 = "top.Weight"() : () -> tensor<1x1x1x128xsi8, 1099512972304 : i64> loc(#loc367)
      %194:3 = "tpu.Group"(%181#1, %181#2, %184) ({
        %302 = "tpu.Load"(%186) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 0, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099522761264 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc371)
        %303 = "tpu.Load"(%181#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 1, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>, 1638400 : i64>) -> tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>> loc(#loc372)
        %304 = "tpu.Load"(%185) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31744, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099522758704 : i64>) -> tensor<1x256x1x9xi8> loc(#loc373)
        %305 = "tpu.Load"(%181#2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>, 2662400 : i64>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>> loc(#loc374)
        %306 = "tpu.Load"(%187) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32032, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099516881616 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc375)
        %307 = "tpu.Conv2D"(%303, %302, %304) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 17408, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.18725376771653543>> loc(#loc376)
        %308 = "tpu.Load"(%184) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 6, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.23008161811023622>, 0 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.23008161811023622>> loc(#loc377)
        %309 = "tpu.Load"(%188) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32496, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 7, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099519742928 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc378)
        %310 = "tpu.Lut"(%305, %306) {ginfo = #tpu.lg<out_addr = 19968, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 8, stage = 1, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.21073828188976379>> loc(#loc379)
        %311 = "tpu.Load"(%189) {do_bcast = true, ginfo = #tpu.lg<out_addr = 24320, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 9, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099522761008 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc380)
        %312 = "tpu.Lut"(%308, %309) {ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 10, stage = 1, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.23008161811023622>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.24148574566929135>> loc(#loc381)
        %313 = "tpu.Load"(%191) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31232, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 11, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099518002208 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc382)
        %314 = "tpu.Load"(%190) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32416, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 12, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512972432 : i64>) -> tensor<1x64x1x9xi8> loc(#loc383)
        %315 = "tpu.Lut"(%307, %311) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 13, stage = 1, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.18725376771653543>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>> loc(#loc368)
        %316 = "tpu.Load"(%193) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32288, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 14, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x128xsi8, 1099512972304 : i64>) -> tensor<1x1x1x128xsi8> loc(#loc384)
        %317 = "tpu.Load"(%192) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32752, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 15, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x9xi8, 1099522467680 : i64>) -> tensor<1x1x1x9xi8> loc(#loc385)
        %318 = "tpu.Store"(%315) {ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38], h_slice = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], w_idx = [0], w_slice = [40], id = 16, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64> loc(#loc368)
        %319 = "tpu.Conv2D"(%310, %313, %314) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 17, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.21073828188976379>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.2026522503937008>> loc(#loc369)
        %320 = "tpu.Store"(%319) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 18, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.2026522503937008>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64> loc(#loc369)
        %321 = "tpu.Conv2D"(%312, %316, %317) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 19, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.24148574566929135>>, tensor<1x1x1x128xsi8>, tensor<1x1x1x9xi8>) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.2026522503937008>> loc(#loc370)
        %322 = "tpu.Store"(%321) {ginfo = #tpu.lg<out_addr = 24576, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [80], id = 20, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x1x80x80x!quant.uniform<i8:f32, 0.2026522503937008>>) -> tensor<1x1x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 1228800 : i64> loc(#loc370)
        "tpu.Yield"(%318, %320, %322) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64>, tensor<1x1x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 1228800 : i64>) -> () loc(#loc567)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 20, -2, 8, 6, 7, -3, 10, 9, 0, -4, 13, 11, 12, -5, 17, 14, 15, 16, -6, 19, 18, 1, 2], group_type = 0 : i64, hsecs = 20 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13058606771653544>, 1638400 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.24795620866141732>, 2662400 : i64>, tensor<1x128x80x80x!quant.uniform<i8:f32, 0.23008161811023622>, 0 : i64>) -> (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64>, tensor<1x1x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 1228800 : i64>) loc(#loc567)
      %195 = "tpu.Slice"(%194#0, %0, %0, %0, %0) {axes = [], ends = [-1, 128, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64>, none, none, none, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64> loc(#loc386)
      %196 = "tpu.Slice"(%194#0, %0, %0, %0, %0) {axes = [], ends = [-1, 256, -1, -1], offset = [0, 128, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64>, none, none, none, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2457600 : i64> loc(#loc387)
      %197 = "tpu.Concat"(%194#1, %194#2) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64>, tensor<1x1x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 1228800 : i64>) -> tensor<1x65x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64> loc(#loc388)
      %198 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099512993616 : i64> loc(#loc389)
      %199 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099519234448 : i64> loc(#loc390)
      %200 = "tpu.Conv2D"(%196, %199, %198) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2457600 : i64>, tensor<1x128x9x128xsi8, 1099519234448 : i64>, tensor<1x128x1x9xi8, 1099512993616 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066689791338582674>, 614400 : i64> loc(#loc391)
      %201 = "tpu.Reshape"(%197) : (tensor<1x65x80x80x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64>) -> tensor<1x65x6400x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64> loc(#loc392)
      %202 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521566752 : i64> loc(#loc393)
      %203 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099512971152 : i64> loc(#loc394)
      %204 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099512725392 : i64> loc(#loc395)
      %205 = "tpu.Group"(%200) ({
        %302 = "tpu.Load"(%200) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066689791338582674>, 614400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066689791338582674>> loc(#loc397)
        %303 = "tpu.Load"(%202) {do_bcast = true, ginfo = #tpu.lg<out_addr = 18432, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521566752 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc398)
        %304 = "tpu.Load"(%204) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 2, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099512725392 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc399)
        %305 = "tpu.Load"(%203) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099512971152 : i64>) -> tensor<1x128x1x9xi8> loc(#loc400)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 3840, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15, 19, 23, 27, 31, 35], h_slice = [5, 6, 6, 6, 6, 6, 6, 6, 6, 5], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066689791338582674>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.047205344881889766>> loc(#loc401)
        %307 = "tpu.Conv2D"(%306, %304, %305) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.047205344881889766>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>> loc(#loc396)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>, 409600 : i64> loc(#loc396)
        "tpu.Yield"(%308) : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>, 409600 : i64>) -> () loc(#loc396)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 10 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-2, 1], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.066689791338582674>, 614400 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>, 409600 : i64> loc(#loc396)
      %206 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512135312 : i64> loc(#loc402)
      %207 = "top.Weight"() : () -> tensor<1x256x1x9xi8, 1099514160528 : i64> loc(#loc403)
      %208 = "top.Weight"() : () -> tensor<1x256x1x384xsi8, 1099512872848 : i64> loc(#loc404)
      %209 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514753424 : i64> loc(#loc405)
      %210 = "tpu.Group"(%205, %195, %196) ({
        %302 = "tpu.Load"(%205) {do_bcast = false, ginfo = #tpu.lg<out_addr = 17408, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>, 409600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>> loc(#loc407)
        %303 = "tpu.Load"(%206) {do_bcast = true, ginfo = #tpu.lg<out_addr = 27136, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512135312 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc408)
        %304 = "tpu.Load"(%195) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>> loc(#loc409)
        %305 = "tpu.Load"(%196) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2457600 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>> loc(#loc410)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.13334468031496061>> loc(#loc411)
        %307 = "tpu.Load"(%208) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x384xsi8, 1099512872848 : i64>) -> tensor<1x256x1x384xsi8> loc(#loc412)
        %308 = "tpu.Load"(%207) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19968, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x9xi8, 1099514160528 : i64>) -> tensor<1x256x1x9xi8> loc(#loc413)
        %309 = "tpu.Concat"(%304, %305, %306) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [384], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>, multipliers = [101, 101, 123], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [6, 6, 7]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.13334468031496061>>) -> tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13830809448818898>> loc(#loc414)
        %310 = "tpu.Load"(%209) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31232, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514753424 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc415)
        %311 = "tpu.Conv2D"(%309, %307, %308) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 9, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x384x40x40x!quant.uniform<i8:f32, 0.13830809448818898>>, tensor<1x256x1x384xsi8>, tensor<1x256x1x9xi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.14048923307086614>> loc(#loc416)
        %312 = "tpu.Lut"(%311, %310) {ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 10, stage = 1, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.14048923307086614>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>> loc(#loc406)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16, 20, 24, 28, 32, 36], h_slice = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4], w_idx = [0], w_slice = [40], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>, 0 : i64> loc(#loc406)
        "tpu.Yield"(%313) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>, 0 : i64>) -> () loc(#loc406)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 11, -2, 7, 5, 6, -3, 9, 8, -4, 10, 0, 1], group_type = 0 : i64, hsecs = 10 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.11794778897637796>, 409600 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2252800 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.21924706299212599>, 2457600 : i64>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>, 0 : i64> loc(#loc406)
      %211 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099512133008 : i64> loc(#loc417)
      %212 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099514163600 : i64> loc(#loc418)
      %213 = "tpu.Conv2D"(%210, %212, %211) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [2, 2], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>, 0 : i64>, tensor<1x256x9x256xsi8, 1099514163600 : i64>, tensor<1x256x1x9xi8, 1099512133008 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.20517653464566929>, 1440000 : i64> loc(#loc419)
      %214 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099512132432 : i64> loc(#loc420)
      %215 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x256xsi8, 1099511983248 : i64> loc(#loc421)
      %216 = "tpu.Conv2D"(%210, %215, %214) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>, 0 : i64>, tensor<1x64x9x256xsi8, 1099511983248 : i64>, tensor<1x64x1x9xi8, 1099512132432 : i64>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.19943130708661416>, 716800 : i64> loc(#loc422)
      %217 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099512973520 : i64> loc(#loc423)
      %218 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x256xsi8, 1099513584848 : i64> loc(#loc424)
      %219 = "tpu.Conv2D"(%210, %218, %217) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13094597559055118>, 0 : i64>, tensor<1x128x9x256xsi8, 1099513584848 : i64>, tensor<1x128x1x9xi8, 1099512973520 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.20920530472440946>, 512000 : i64> loc(#loc425)
      %220 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511982992 : i64> loc(#loc426)
      %221 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099518011808 : i64> loc(#loc427)
      %222 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099517919584 : i64> loc(#loc428)
      %223:3 = "tpu.Group"(%213, %216, %219) ({
        %302 = "tpu.Load"(%213) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.20517653464566929>, 1440000 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.20517653464566929>> loc(#loc432)
        %303 = "tpu.Load"(%220) {do_bcast = true, ginfo = #tpu.lg<out_addr = 31360, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511982992 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc433)
        %304 = "tpu.Load"(%216) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18688, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30], h_slice = [10, 10, 10, 10], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.19943130708661416>, 716800 : i64>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.19943130708661416>> loc(#loc434)
        %305 = "tpu.Load"(%221) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10240, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099518011808 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc435)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 6400, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 4, stage = 1, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.20517653464566929>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>> loc(#loc429)
        %307 = "tpu.Load"(%219) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30], h_slice = [10, 10, 10, 10], w_idx = [0], w_slice = [40], id = 5, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.20920530472440946>, 512000 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.20920530472440946>> loc(#loc436)
        %308 = "tpu.Load"(%222) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9984, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099517919584 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc437)
        %309 = "tpu.Store"(%306) {ginfo = #tpu.lg<out_addr = 6400, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 7, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>, 1235200 : i64> loc(#loc429)
        %310 = "tpu.Lut"(%304, %305) {ginfo = #tpu.lg<out_addr = 28160, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30], h_slice = [10, 10, 10, 10], w_idx = [0], w_slice = [40], id = 8, stage = 1, group_type = 0>} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.19943130708661416>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>> loc(#loc430)
        %311 = "tpu.Store"(%310) {ginfo = #tpu.lg<out_addr = 28160, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30], h_slice = [10, 10, 10, 10], w_idx = [0], w_slice = [40], id = 9, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>, 1337600 : i64> loc(#loc430)
        %312 = "tpu.Lut"(%307, %308) {ginfo = #tpu.lg<out_addr = 12288, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30], h_slice = [10, 10, 10, 10], w_idx = [0], w_slice = [40], id = 10, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.20920530472440946>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.24321168740157481>> loc(#loc431)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 12288, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 10, 20, 30], h_slice = [10, 10, 10, 10], w_idx = [0], w_slice = [40], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.24321168740157481>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.24321168740157481>, 307200 : i64> loc(#loc431)
        "tpu.Yield"(%309, %311, %313) : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>, 1235200 : i64>, tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>, 1337600 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.24321168740157481>, 307200 : i64>) -> () loc(#loc568)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 11, -2, 8, 5, 6, 7, -3, 10, 9, 0, 1], group_type = 0 : i64, hsecs = 4 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.20517653464566929>, 1440000 : i64>, tensor<1x64x40x40x!quant.uniform<i8:f32, 0.19943130708661416>, 716800 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.20920530472440946>, 512000 : i64>) -> (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>, 1235200 : i64>, tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>, 1337600 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.24321168740157481>, 307200 : i64>) loc(#loc568)
      %224 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511982416 : i64> loc(#loc438)
      %225 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099517920608 : i64> loc(#loc439)
      %226:2 = "tpu.Group"(%223#0, %127#0, %223#1) ({
        %302 = "tpu.Load"(%223#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>, 1235200 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>> loc(#loc442)
        %303 = "tpu.Load"(%127#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 1, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>, 3686400 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>> loc(#loc443)
        %304 = "tpu.Load"(%223#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 7, 15, 23, 31], h_slice = [9, 10, 10, 10, 9], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>, 1337600 : i64>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>> loc(#loc444)
        %305 = "tpu.Load"(%225) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099517920608 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc445)
        %306 = "tpu.Load"(%224) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7680, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511982416 : i64>) -> tensor<1x64x1x9xi8> loc(#loc446)
        %307 = "tpu.Concat"(%302, %303) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 5, stage = 1, group_type = 0>, multipliers = [68, 99], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 10]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.23419697716535431>> loc(#loc440)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 0, out_size = 7680, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 6, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.23419697716535431>>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.23419697716535431>, 0 : i64> loc(#loc440)
        %309 = "tpu.Conv2D"(%304, %305, %306) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 13312, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>> loc(#loc441)
        %310 = "tpu.Store"(%309) {ginfo = #tpu.lg<out_addr = 13312, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 8, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>, 716800 : i64> loc(#loc441)
        "tpu.Yield"(%308, %310) : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.23419697716535431>, 0 : i64>, tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>, 716800 : i64>) -> () loc(#loc569)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 4, 8, -2, 7, 6, 0, 1], group_type = 0 : i64, hsecs = 5 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12611039763779527>, 1235200 : i64>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.022859354330708664>, 3686400 : i64>, tensor<1x64x40x40x!quant.uniform<i8:f32, 0.13253372125984253>, 1337600 : i64>) -> (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.23419697716535431>, 0 : i64>, tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>, 716800 : i64>) loc(#loc569)
      %227 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099512131280 : i64> loc(#loc447)
      %228 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x128xsi8, 1099511834960 : i64> loc(#loc448)
      %229 = "tpu.Conv2D"(%223#2, %228, %227) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.24321168740157481>, 307200 : i64>, tensor<1x128x9x128xsi8, 1099511834960 : i64>, tensor<1x128x1x9xi8, 1099512131280 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.41480553149606303>, 1235200 : i64> loc(#loc449)
      %230 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099511830352 : i64> loc(#loc450)
      %231 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x768xsi8, 1099521964640 : i64> loc(#loc451)
      %232 = "tpu.Conv2D"(%226#0, %231, %230) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.23419697716535431>, 0 : i64>, tensor<1x512x1x768xsi8, 1099521964640 : i64>, tensor<1x512x1x9xi8, 1099511830352 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21809884881889763>, 512000 : i64> loc(#loc452)
      %233 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514756240 : i64> loc(#loc453)
      %234 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099521360992 : i64> loc(#loc454)
      %235 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511830096 : i64> loc(#loc455)
      %236:3 = "tpu.Group"(%226#1, %229, %232) ({
        %302 = "tpu.Load"(%226#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>, 716800 : i64>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>> loc(#loc459)
        %303 = "tpu.Load"(%233) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10496, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099514756240 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc460)
        %304 = "tpu.Load"(%229) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.41480553149606303>, 1235200 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.41480553149606303>> loc(#loc461)
        %305 = "tpu.Load"(%234) {do_bcast = true, ginfo = #tpu.lg<out_addr = 10240, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099521360992 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc462)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 17408, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 4, stage = 1, group_type = 0>} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>> loc(#loc456)
        %307 = "tpu.Load"(%232) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 5, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21809884881889763>, 512000 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21809884881889763>> loc(#loc463)
        %308 = "tpu.Load"(%235) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511830096 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc464)
        %309 = "tpu.Store"(%306) {ginfo = #tpu.lg<out_addr = 17408, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>, 204800 : i64> loc(#loc456)
        %310 = "tpu.Lut"(%304, %305) {ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 8, stage = 1, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.41480553149606303>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>> loc(#loc457)
        %311 = "tpu.Store"(%310) {ginfo = #tpu.lg<out_addr = 20480, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 8, 16, 24, 32], h_slice = [8, 8, 8, 8, 8], w_idx = [0], w_slice = [40], id = 9, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>, 0 : i64> loc(#loc457)
        %312 = "tpu.Lut"(%307, %308) {ginfo = #tpu.lg<out_addr = 5120, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 10, stage = 1, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21809884881889763>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>> loc(#loc458)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 5120, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 11, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64> loc(#loc458)
        "tpu.Yield"(%309, %311, %313) : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>, 204800 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>, 0 : i64>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64>) -> () loc(#loc570)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 11, -2, 8, 5, 6, 7, -3, 10, 9, 0, 1], group_type = 0 : i64, hsecs = 5 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.3307940566929134>, 716800 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.41480553149606303>, 1235200 : i64>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21809884881889763>, 512000 : i64>) -> (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>, 204800 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>, 0 : i64>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64>) loc(#loc570)
      %237 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511792656 : i64> loc(#loc465)
      %238 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099518007712 : i64> loc(#loc466)
      %239 = "top.Weight"() : () -> tensor<1x1x1x9xi8, 1099518053152 : i64> loc(#loc467)
      %240 = "top.Weight"() : () -> tensor<1x1x1x128xsi8, 1099517472224 : i64> loc(#loc468)
      %241:2 = "tpu.Group"(%236#0, %236#1) ({
        %302 = "tpu.Load"(%236#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>, 204800 : i64>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>> loc(#loc471)
        %303 = "tpu.Load"(%238) {do_bcast = false, ginfo = #tpu.lg<out_addr = 13600, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099518007712 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc472)
        %304 = "tpu.Load"(%237) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14240, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511792656 : i64>) -> tensor<1x64x1x9xi8> loc(#loc473)
        %305 = "tpu.Load"(%236#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>, 0 : i64>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>> loc(#loc474)
        %306 = "tpu.Load"(%240) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14112, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x128xsi8, 1099517472224 : i64>) -> tensor<1x1x1x128xsi8> loc(#loc475)
        %307 = "tpu.Load"(%239) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14320, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x9xi8, 1099518053152 : i64>) -> tensor<1x1x1x9xi8> loc(#loc476)
        %308 = "tpu.Conv2D"(%302, %303, %304) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 6, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.2026522503937008>> loc(#loc469)
        %309 = "tpu.Store"(%308) {ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 7, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.2026522503937008>>) -> tensor<1x64x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64> loc(#loc469)
        %310 = "tpu.Conv2D"(%305, %306, %307) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 800, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 8, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>>, tensor<1x1x1x128xsi8>, tensor<1x1x1x9xi8>) -> tensor<1x1x40x40x!quant.uniform<i8:f32, 0.2026522503937008>> loc(#loc470)
        %311 = "tpu.Store"(%310) {ginfo = #tpu.lg<out_addr = 12800, out_size = 800, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0, 20], h_slice = [20, 20], w_idx = [0], w_slice = [40], id = 9, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x1x40x40x!quant.uniform<i8:f32, 0.2026522503937008>>) -> tensor<1x1x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 648400 : i64> loc(#loc470)
        "tpu.Yield"(%309, %311) : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64>, tensor<1x1x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 648400 : i64>) -> () loc(#loc571)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 6, 3, 4, 5, 9, -2, 8, 7, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.24419514488188976>, 204800 : i64>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.33447502913385824>, 0 : i64>) -> (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64>, tensor<1x1x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 648400 : i64>) loc(#loc571)
      %242 = "tpu.Slice"(%236#2, %0, %0, %0, %0) {axes = [], ends = [-1, 256, -1, -1], offset = [0, 0, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64>, none, none, none, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64> loc(#loc477)
      %243 = "tpu.Slice"(%236#2, %0, %0, %0, %0) {axes = [], ends = [-1, 512, -1, -1], offset = [0, 256, 0, 0], steps = [1, 1, 1, 1]} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64>, none, none, none, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 409600 : i64> loc(#loc478)
      %244 = "tpu.Concat"(%241#0, %241#1) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64>, tensor<1x1x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 648400 : i64>) -> tensor<1x65x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64> loc(#loc479)
      %245 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099511790336 : i64> loc(#loc480)
      %246 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099514947536 : i64> loc(#loc481)
      %247 = "tpu.Conv2D"(%243, %246, %245) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 409600 : i64>, tensor<1x256x9x256xsi8, 1099514947536 : i64>, tensor<1x256x1x9xi8, 1099511790336 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12402906929133858>, 650000 : i64> loc(#loc482)
      %248 = "tpu.Reshape"(%244) : (tensor<1x65x40x40x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64>) -> tensor<1x65x1600x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64> loc(#loc483)
      %249 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099514011664 : i64> loc(#loc484)
      %250 = "tpu.Lut"(%247, %249) : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.12402906929133858>, 650000 : i64>, tensor<1x1x1x256xsi8, 1099514011664 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068294201574803148>, 0 : i64> loc(#loc485)
      %251 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x1x9xi8, 1099516879312 : i64> loc(#loc486)
      %252 = "top.Weight"() {do_compress = true} : () -> tensor<1x256x9x256xsi8, 1099512135568 : i64> loc(#loc487)
      %253 = "tpu.Conv2D"(%250, %252, %251) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.068294201574803148>, 0 : i64>, tensor<1x256x9x256xsi8, 1099512135568 : i64>, tensor<1x256x1x9xi8, 1099516879312 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10184211417322835>, 650000 : i64> loc(#loc488)
      %254 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511790080 : i64> loc(#loc489)
      %255 = "tpu.Group"(%253, %242, %243) ({
        %302 = "tpu.Load"(%253) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10184211417322835>, 650000 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10184211417322835>> loc(#loc491)
        %303 = "tpu.Load"(%254) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511790080 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc492)
        %304 = "tpu.Load"(%242) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>> loc(#loc493)
        %305 = "tpu.Load"(%243) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 3, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 409600 : i64>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>> loc(#loc494)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 20480, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 4, stage = 1, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10184211417322835>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.14138108425196849>> loc(#loc495)
        %307 = "tpu.Concat"(%304, %305, %306) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10752, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 5, stage = 1, group_type = 0>, multipliers = [114, 114, 74], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [7, 7, 7]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.14138108425196849>>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.2418954220472441>> loc(#loc490)
        %308 = "tpu.Store"(%307) {ginfo = #tpu.lg<out_addr = 0, out_size = 10752, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [768], d_idx = [0], d_slice = [1], h_idx = [0, 5, 10, 15], h_slice = [5, 5, 5, 5], w_idx = [0], w_slice = [20], id = 6, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.2418954220472441>>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.2418954220472441>, 0 : i64> loc(#loc490)
        "tpu.Yield"(%308) : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.2418954220472441>, 0 : i64>) -> () loc(#loc490)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 6, -2, 5, 0, 1], group_type = 0 : i64, hsecs = 4 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10184211417322835>, 650000 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 307200 : i64>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.21550783070866139>, 409600 : i64>) -> tensor<1x768x20x20x!quant.uniform<i8:f32, 0.2418954220472441>, 0 : i64> loc(#loc490)
      %256 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x9xi8, 1099511784896 : i64> loc(#loc496)
      %257 = "top.Weight"() {do_compress = true} : () -> tensor<1x512x1x768xsi8, 1099520949984 : i64> loc(#loc497)
      %258 = "tpu.Conv2D"(%255, %257, %256) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x768x20x20x!quant.uniform<i8:f32, 0.2418954220472441>, 0 : i64>, tensor<1x512x1x768xsi8, 1099520949984 : i64>, tensor<1x512x1x9xi8, 1099511784896 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.20640631574803148>, 307200 : i64> loc(#loc498)
      %259 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511784640 : i64> loc(#loc499)
      %260 = "tpu.Lut"(%258, %259) : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.20640631574803148>, 307200 : i64>, tensor<1x1x1x256xsi8, 1099511784640 : i64>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.23920146614173229>, 0 : i64> loc(#loc500)
      %261 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x1x9xi8, 1099511784064 : i64> loc(#loc501)
      %262 = "top.Weight"() {do_compress = true} : () -> tensor<1x64x9x512xsi8, 1099516584144 : i64> loc(#loc502)
      %263 = "tpu.Conv2D"(%260, %262, %261) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.23920146614173229>, 0 : i64>, tensor<1x64x9x512xsi8, 1099516584144 : i64>, tensor<1x64x1x9xi8, 1099511784064 : i64>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.16071241496062993>, 778000 : i64> loc(#loc503)
      %264 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x1x9xi8, 1099511782912 : i64> loc(#loc504)
      %265 = "top.Weight"() {do_compress = true} : () -> tensor<1x128x9x512xsi8, 1099516881872 : i64> loc(#loc505)
      %266 = "tpu.Conv2D"(%260, %265, %264) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.23920146614173229>, 0 : i64>, tensor<1x128x9x512xsi8, 1099516881872 : i64>, tensor<1x128x1x9xi8, 1099511782912 : i64>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12548696692913386>, 701200 : i64> loc(#loc506)
      %267 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512974928 : i64> loc(#loc507)
      %268 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511782656 : i64> loc(#loc508)
      %269 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099512130704 : i64> loc(#loc509)
      %270 = "top.Weight"() : () -> tensor<1x64x9x64xsi8, 1099511793232 : i64> loc(#loc510)
      %271 = "top.Weight"() : () -> tensor<1x128x1x9xi8, 1099511781504 : i64> loc(#loc511)
      %272 = "top.Weight"() : () -> tensor<1x128x9x128xsi8, 1099511634048 : i64> loc(#loc512)
      %273:2 = "tpu.Group"(%263, %266) ({
        %302 = "tpu.Load"(%263) {do_bcast = false, ginfo = #tpu.lg<out_addr = 26368, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15], h_slice = [5, 6, 6, 6, 5], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.16071241496062993>, 778000 : i64>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.16071241496062993>> loc(#loc515)
        %303 = "tpu.Load"(%267) {do_bcast = true, ginfo = #tpu.lg<out_addr = 27648, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099512974928 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc516)
        %304 = "tpu.Load"(%266) {do_bcast = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15], h_slice = [5, 6, 6, 6, 5], w_idx = [0], w_slice = [20], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12548696692913386>, 701200 : i64>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12548696692913386>> loc(#loc517)
        %305 = "tpu.Load"(%268) {do_bcast = true, ginfo = #tpu.lg<out_addr = 27392, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511782656 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc518)
        %306 = "tpu.Load"(%269) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28048, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099512130704 : i64>) -> tensor<1x64x1x9xi8> loc(#loc519)
        %307 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 30720, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15], h_slice = [5, 6, 6, 6, 5], w_idx = [0], w_slice = [20], id = 5, stage = 1, group_type = 0>} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.16071241496062993>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.14739053070866143>> loc(#loc520)
        %308 = "tpu.Load"(%270) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [64], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x9x64xsi8, 1099511793232 : i64>) -> tensor<1x64x9x64xsi8> loc(#loc521)
        %309 = "tpu.Lut"(%304, %305) {ginfo = #tpu.lg<out_addr = 28672, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 3, 7, 11, 15], h_slice = [5, 6, 6, 6, 5], w_idx = [0], w_slice = [20], id = 7, stage = 1, group_type = 0>} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12548696692913386>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12755468503937009>> loc(#loc522)
        %310 = "tpu.Load"(%272) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [9], w_idx = [0], w_slice = [128], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x9x128xsi8, 1099511634048 : i64>) -> tensor<1x128x9x128xsi8> loc(#loc523)
        %311 = "tpu.Load"(%271) {do_bcast = false, ginfo = #tpu.lg<out_addr = 27904, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x9xi8, 1099511781504 : i64>) -> tensor<1x128x1x9xi8> loc(#loc524)
        %312 = "tpu.Conv2D"(%307, %308, %306) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 18432, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 10, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.14739053070866143>>, tensor<1x64x9x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>> loc(#loc513)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 18432, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 11, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>, 752400 : i64> loc(#loc513)
        %314 = "tpu.Conv2D"(%309, %310, %311) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 25088, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 12, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12755468503937009>>, tensor<1x128x9x128xsi8>, tensor<1x128x1x9xi8>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>> loc(#loc514)
        %315 = "tpu.Store"(%314) {ginfo = #tpu.lg<out_addr = 25088, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 4, 8, 12, 16], h_slice = [4, 4, 4, 4, 4], w_idx = [0], w_slice = [20], id = 13, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>, 650000 : i64> loc(#loc514)
        "tpu.Yield"(%313, %315) : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>, 752400 : i64>, tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>, 650000 : i64>) -> () loc(#loc572)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 2, 3, 13, 4, -2, 7, 6, -3, 10, 8, 9, -4, 12, 11, 0, 1], group_type = 0 : i64, hsecs = 5 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.16071241496062993>, 778000 : i64>, tensor<1x128x20x20x!quant.uniform<i8:f32, 0.12548696692913386>, 701200 : i64>) -> (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>, 752400 : i64>, tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>, 650000 : i64>) loc(#loc572)
      %274 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511633792 : i64> loc(#loc525)
      %275 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099511633536 : i64> loc(#loc526)
      %276 = "top.Weight"() : () -> tensor<1x64x1x9xi8, 1099511789504 : i64> loc(#loc527)
      %277 = "top.Weight"() : () -> tensor<1x64x1x64xsi8, 1099511629440 : i64> loc(#loc528)
      %278 = "top.Weight"() : () -> tensor<1x1x1x9xi8, 1099511792640 : i64> loc(#loc529)
      %279 = "top.Weight"() : () -> tensor<1x1x1x128xsi8, 1099511629312 : i64> loc(#loc530)
      %280:2 = "tpu.Group"(%273#0, %273#1) ({
        %302 = "tpu.Load"(%273#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>, 752400 : i64>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>> loc(#loc533)
        %303 = "tpu.Load"(%274) {do_bcast = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511633792 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc534)
        %304 = "tpu.Load"(%273#1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 2, stage = 1, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>, 650000 : i64>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>> loc(#loc535)
        %305 = "tpu.Load"(%275) {do_bcast = true, ginfo = #tpu.lg<out_addr = 9600, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 3, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 1099511633536 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc536)
        %306 = "tpu.Lut"(%302, %303) {ginfo = #tpu.lg<out_addr = 6400, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 4, stage = 1, group_type = 0>} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.15625583700787402>> loc(#loc537)
        %307 = "tpu.Load"(%277) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xsi8, 1099511629440 : i64>) -> tensor<1x64x1x64xsi8> loc(#loc538)
        %308 = "tpu.Load"(%276) {do_bcast = false, ginfo = #tpu.lg<out_addr = 9856, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 6, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9xi8, 1099511789504 : i64>) -> tensor<1x64x1x9xi8> loc(#loc539)
        %309 = "tpu.Lut"(%304, %305) {ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 7, stage = 1, group_type = 0>} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x20x20x!quant.uniform<i8:f32, 0.4202878811023622>> loc(#loc540)
        %310 = "tpu.Load"(%279) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 8, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x128xsi8, 1099511629312 : i64>) -> tensor<1x1x1x128xsi8> loc(#loc541)
        %311 = "tpu.Load"(%278) {do_bcast = false, ginfo = #tpu.lg<out_addr = 23680, out_size = 9, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9], id = 9, stage = 1, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x9xi8, 1099511792640 : i64>) -> tensor<1x1x1x9xi8> loc(#loc542)
        %312 = "tpu.Conv2D"(%306, %307, %308) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 10, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.15625583700787402>>, tensor<1x64x1x64xsi8>, tensor<1x64x1x9xi8>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.2026522503937008>> loc(#loc531)
        %313 = "tpu.Store"(%312) {ginfo = #tpu.lg<out_addr = 12288, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 11, stage = 1, group_type = 0>, support_compress = true} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.2026522503937008>>) -> tensor<1x64x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64> loc(#loc531)
        %314 = "tpu.Conv2D"(%309, %310, %311) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 12, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x20x20x!quant.uniform<i8:f32, 0.4202878811023622>>, tensor<1x1x1x128xsi8>, tensor<1x1x1x9xi8>) -> tensor<1x1x20x20x!quant.uniform<i8:f32, 0.2026522503937008>> loc(#loc532)
        %315 = "tpu.Store"(%314) {ginfo = #tpu.lg<out_addr = 24576, out_size = 400, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 13, stage = 2, group_type = 0>, support_compress = true} : (tensor<1x1x20x20x!quant.uniform<i8:f32, 0.2026522503937008>>) -> tensor<1x1x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 726800 : i64> loc(#loc532)
        "tpu.Yield"(%313, %315) : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64>, tensor<1x1x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 726800 : i64>) -> () loc(#loc573)
      }) {csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 2, 3, 13, -2, 7, 5, 6, -3, 10, 8, 9, 0, 1, -4, 12, 11], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.21239462440944881>, 752400 : i64>, tensor<1x128x20x20x!quant.uniform<i8:f32, 0.43941393700787407>, 650000 : i64>) -> (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64>, tensor<1x1x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 726800 : i64>) loc(#loc573)
      %281 = "tpu.Concat"(%280#0, %280#1) {axis = 1 : si32, do_relu = false, multipliers = [1, 1], only_merge = true, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0]} : (tensor<1x64x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64>, tensor<1x1x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 726800 : i64>) -> tensor<1x65x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64> loc(#loc543)
      %282 = "tpu.Reshape"(%281) : (tensor<1x65x20x20x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64>) -> tensor<1x65x400x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64> loc(#loc544)
      %283 = "tpu.Concat"(%201, %248, %282) {axis = 2 : si32, do_relu = false, multipliers = [1, 1, 1], only_merge = false, relu_limit = -1.000000e+00 : f64, rshifts = [0, 0, 0]} : (tensor<1x65x6400x!quant.uniform<i8:f32, 0.2026522503937008>, 819200 : i64>, tensor<1x65x1600x!quant.uniform<i8:f32, 0.2026522503937008>, 546000 : i64>, tensor<1x65x400x!quant.uniform<i8:f32, 0.2026522503937008>, 701200 : i64>) -> tensor<1x65x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64> loc(#loc545)
      %284 = "tpu.Slice"(%283, %0, %0, %0, %0) {axes = [], ends = [-1, 65, -1], offset = [0, 64, 0], steps = [1, 1, 1]} : (tensor<1x65x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64>, none, none, none, none) -> tensor<1x1x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 537600 : i64> loc(#loc546)
      %285 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 1099512973008 : i64> loc(#loc547)
      %286 = "tpu.Lut"(%284, %285) : (tensor<1x1x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 537600 : i64>, tensor<1x1x1x256xsi8, 1099512973008 : i64>) -> tensor<1x1x8400x!quant.uniform<i8:f32, 0.0078740157480314959>, 2150400 : i64> loc(#loc548)
      %287 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099514757072 : i64> loc(#loc549)
      %288 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099511628800 : i64> loc(#loc550)
      %289 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099511628288 : i64> loc(#loc551)
      %290 = "top.Weight"() : () -> tensor<1x1x32x8xbf16, 1099511627776 : i64> loc(#loc552)
      %291 = "top.Weight"() {do_compress = true} : () -> tensor<1x1x1x5xi8, 1099520949968 : i64> loc(#loc553)
      %292 = "top.Weight"() {do_compress = true} : () -> tensor<1x1x1x16xsi8, 1099517472208 : i64> loc(#loc554)
      %293 = "tpu.Slice"(%283, %0, %0, %0, %0) {axes = [], ends = [-1, 64, -1], offset = [0, 0, 0], steps = [1, 1, 1]} : (tensor<1x65x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64>, none, none, none, none) -> tensor<1x64x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64> loc(#loc555)
      %294 = "tpu.Reshape"(%293) : (tensor<1x64x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64>) -> tensor<1x4x16x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64> loc(#loc556)
      %295 = "tpu.Cast"(%294) {with_scale = true} : (tensor<1x4x16x8400x!quant.uniform<i8:f32, 0.2026522503937008>, 0 : i64>) -> tensor<1x4x16x8400xbf16, 1075200 : i64> loc(#loc557)
      %296 = "tpu.Softmax"(%295, %287, %288, %289, %290, %0) {axis = 2 : si32, beta = 1.000000e+00 : f64, log = false} : (tensor<1x4x16x8400xbf16, 1075200 : i64>, tensor<1x1x32x8xbf16, 1099514757072 : i64>, tensor<1x1x32x8xbf16, 1099511628800 : i64>, tensor<1x1x32x8xbf16, 1099511628288 : i64>, tensor<1x1x32x8xbf16, 1099511627776 : i64>, none) -> tensor<1x4x16x8400x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 0 : i64> loc(#loc558)
      %297 = "tpu.Cast"(%296) {with_scale = true} : (tensor<1x4x16x8400x!quant.calibrated<bf16<-1.000000e+00:1.000000e+00>>, 0 : i64>) -> tensor<1x4x16x8400x!quant.uniform<i8:bf16, 0.0078740157480314959>, 1075200 : i64> loc(#loc559)
      %298 = "tpu.Permute"(%297, %0) {order = [0, 2, 1, 3]} : (tensor<1x4x16x8400x!quant.uniform<i8:bf16, 0.0078740157480314959>, 1075200 : i64>, none) -> tensor<1x16x4x8400x!quant.uniform<i8:bf16, 0.0078740157480314959>, 0 : i64> loc(#loc560)
      %299 = "tpu.Conv2D"(%298, %292, %291) {coeff_merged = false, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode QDM>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = false} : (tensor<1x16x4x8400x!quant.uniform<i8:bf16, 0.0078740157480314959>, 0 : i64>, tensor<1x1x1x16xsi8, 1099517472208 : i64>, tensor<1x1x1x5xi8, 1099520949968 : i64>) -> tensor<1x1x4x8400x!quant.uniform<i8:f32, 0.11501569055118109>, 537600 : i64> loc(#loc561)
      %300 = "tpu.Cast"(%299) {with_scale = true} : (tensor<1x1x4x8400x!quant.uniform<i8:f32, 0.11501569055118109>, 537600 : i64>) -> tensor<1x1x4x8400xf32, 4398046511104 : i64> loc(#loc562)
      %301 = "tpu.Cast"(%286) {with_scale = true} : (tensor<1x1x8400x!quant.uniform<i8:f32, 0.0078740157480314959>, 2150400 : i64>) -> tensor<1x1x8400xf32, 5497558138880 : i64> loc(#loc563)
      return %301, %300 : tensor<1x1x8400xf32, 5497558138880 : i64>, tensor<1x1x4x8400xf32, 4398046511104 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc3 = loc("/model.0/conv/Conv_output_0_Conv_bias_packed")
#loc4 = loc("/model.0/conv/Conv_output_0_Conv_filter_reordered")
#loc5 = loc("/model.0/act/Mul_output_0_Mul_table")
#loc6 = loc("/model.1/conv/Conv_output_0_Conv_bias_packed")
#loc7 = loc("/model.1/conv/Conv_output_0_Conv_filter_reordered")
#loc8 = loc("/model.1/act/Mul_output_0_Mul_table")
#loc9 = loc("/model.1/act/Mul_output_0_Mul")
#loc10 = loc("load_0")
#loc11 = loc("load_/model.0/conv/Conv_output_0_Conv_filter_reordered")
#loc12 = loc("load_/model.0/conv/Conv_output_0_Conv_bias_packed")
#loc13 = loc("load_/model.0/act/Mul_output_0_Mul_table")
#loc14 = loc("/model.0/conv/Conv_output_0_Conv")
#loc15 = loc("load_/model.1/conv/Conv_output_0_Conv_filter_reordered")
#loc16 = loc("load_/model.1/conv/Conv_output_0_Conv_bias_packed")
#loc17 = loc("/model.0/act/Mul_output_0_Mul")
#loc18 = loc("load_/model.1/act/Mul_output_0_Mul_table")
#loc19 = loc("/model.1/conv/Conv_output_0_Conv")
#loc20 = loc("/model.2/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc21 = loc("/model.2/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc22 = loc("/model.2/cv1/act/Mul_output_0_Mul_table")
#loc23 = loc("/model.2/cv1/act/Mul_output_0_Mul")
#loc24 = loc("load_/model.1/act/Mul_output_0_Mul")
#loc25 = loc("load_/model.2/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc26 = loc("load_/model.2/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc27 = loc("load_/model.2/cv1/act/Mul_output_0_Mul_table")
#loc28 = loc("/model.2/cv1/conv/Conv_output_0_Conv")
#loc29 = loc("/model.2/Split_output_0_Split")
#loc30 = loc("/model.2/Split_output_1_Split")
#loc31 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc32 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc33 = loc("/model.2/m.0/cv1/conv/Conv_output_0_Conv")
#loc34 = loc("/model.2/m.0/cv1/act/Mul_output_0_Mul_table")
#loc35 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc36 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc37 = loc("/model.2/m.0/cv2/act/Mul_output_0_Mul_table")
#loc38 = loc("/model.2/m.0/cv2/act/Mul_output_0_Mul")
#loc39 = loc("load_/model.2/m.0/cv1/conv/Conv_output_0_Conv")
#loc40 = loc("load_/model.2/m.0/cv1/act/Mul_output_0_Mul_table")
#loc41 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc42 = loc("load_/model.2/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc43 = loc("/model.2/m.0/cv1/act/Mul_output_0_Mul")
#loc44 = loc("load_/model.2/m.0/cv2/act/Mul_output_0_Mul_table")
#loc45 = loc("/model.2/m.0/cv2/conv/Conv_output_0_Conv")
#loc46 = loc("/model.2/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc47 = loc("/model.2/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc48 = loc("/model.2/cv2/act/Mul_output_0_Mul_table")
#loc49 = loc("/model.2/cv2/act/Mul_output_0_Mul")
#loc50 = loc("load_/model.2/Split_output_1_Split")
#loc51 = loc("load_/model.2/m.0/cv2/act/Mul_output_0_Mul")
#loc52 = loc("load_/model.2/Split_output_0_Split")
#loc53 = loc("/model.2/m.0/Add_output_0_Add")
#loc54 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc55 = loc("load_/model.2/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc56 = loc("/model.2/Concat_output_0_Concat")
#loc57 = loc("load_/model.2/cv2/act/Mul_output_0_Mul_table")
#loc58 = loc("/model.2/cv2/conv/Conv_output_0_Conv")
#loc59 = loc("/model.3/conv/Conv_output_0_Conv_bias_packed")
#loc60 = loc("/model.3/conv/Conv_output_0_Conv_filter_reordered")
#loc61 = loc("/model.3/act/Mul_output_0_Mul_table")
#loc62 = loc("/model.4/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc63 = loc("/model.4/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc64 = loc("/model.4/cv1/act/Mul_output_0_Mul_table")
#loc65 = loc("/model.4/cv1/act/Mul_output_0_Mul")
#loc66 = loc("load_/model.3/conv/Conv_output_0_Conv_filter_reordered")
#loc67 = loc("load_/model.2/cv2/act/Mul_output_0_Mul")
#loc68 = loc("load_/model.3/conv/Conv_output_0_Conv_bias_packed")
#loc69 = loc("load_/model.3/act/Mul_output_0_Mul_table")
#loc70 = loc("/model.3/conv/Conv_output_0_Conv")
#loc71 = loc("load_/model.4/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc72 = loc("load_/model.4/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc73 = loc("/model.3/act/Mul_output_0_Mul")
#loc74 = loc("load_/model.4/cv1/act/Mul_output_0_Mul_table")
#loc75 = loc("/model.4/cv1/conv/Conv_output_0_Conv")
#loc76 = loc("/model.4/Split_output_0_Split")
#loc77 = loc("/model.4/Split_output_1_Split")
#loc78 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc79 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc80 = loc("/model.4/m.0/cv1/conv/Conv_output_0_Conv")
#loc81 = loc("/model.4/m.0/cv1/act/Mul_output_0_Mul_table")
#loc82 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc83 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc84 = loc("/model.4/m.0/cv2/conv/Conv_output_0_Conv")
#loc85 = loc("load_/model.4/m.0/cv1/conv/Conv_output_0_Conv")
#loc86 = loc("load_/model.4/m.0/cv1/act/Mul_output_0_Mul_table")
#loc87 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc88 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc89 = loc("/model.4/m.0/cv1/act/Mul_output_0_Mul")
#loc90 = loc("/model.4/m.0/cv2/act/Mul_output_0_Mul_table")
#loc91 = loc("/model.4/m.0/Add_output_0_Add")
#loc92 = loc("load_/model.4/m.0/cv2/conv/Conv_output_0_Conv")
#loc93 = loc("load_/model.4/m.0/cv2/act/Mul_output_0_Mul_table")
#loc94 = loc("load_/model.4/Split_output_1_Split")
#loc95 = loc("/model.4/m.0/cv2/act/Mul_output_0_Mul")
#loc96 = loc("/model.4/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc97 = loc("/model.4/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc98 = loc("/model.4/m.1/cv1/conv/Conv_output_0_Conv")
#loc99 = loc("/model.4/m.1/cv1/act/Mul_output_0_Mul_table")
#loc100 = loc("/model.4/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc101 = loc("/model.4/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc102 = loc("/model.4/m.1/cv2/conv/Conv_output_0_Conv")
#loc103 = loc("load_/model.4/m.1/cv1/conv/Conv_output_0_Conv")
#loc104 = loc("load_/model.4/m.1/cv1/act/Mul_output_0_Mul_table")
#loc105 = loc("load_/model.4/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc106 = loc("load_/model.4/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc107 = loc("/model.4/m.1/cv1/act/Mul_output_0_Mul")
#loc108 = loc("/model.4/m.1/cv2/act/Mul_output_0_Mul_table")
#loc109 = loc("/model.4/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc110 = loc("/model.4/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc111 = loc("/model.4/cv2/act/Mul_output_0_Mul_table")
#loc112 = loc("/model.4/cv2/act/Mul_output_0_Mul")
#loc113 = loc("load_/model.4/m.1/cv2/conv/Conv_output_0_Conv")
#loc114 = loc("load_/model.4/m.1/cv2/act/Mul_output_0_Mul_table")
#loc115 = loc("load_/model.4/m.0/Add_output_0_Add")
#loc116 = loc("/model.4/m.1/cv2/act/Mul_output_0_Mul")
#loc117 = loc("load_/model.4/Split_output_0_Split")
#loc118 = loc("/model.4/m.1/Add_output_0_Add")
#loc119 = loc("load_/model.4/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc120 = loc("load_/model.4/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc121 = loc("/model.4/Concat_output_0_Concat")
#loc122 = loc("load_/model.4/cv2/act/Mul_output_0_Mul_table")
#loc123 = loc("/model.4/cv2/conv/Conv_output_0_Conv")
#loc124 = loc("/model.5/conv/Conv_output_0_Conv_bias_packed")
#loc125 = loc("/model.5/conv/Conv_output_0_Conv_filter_reordered")
#loc126 = loc("/model.5/conv/Conv_output_0_Conv")
#loc127 = loc("/model.5/act/Mul_output_0_Mul_table")
#loc128 = loc("/model.6/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc129 = loc("/model.6/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc130 = loc("/model.6/cv1/act/Mul_output_0_Mul_table")
#loc131 = loc("/model.6/cv1/act/Mul_output_0_Mul")
#loc132 = loc("load_/model.5/conv/Conv_output_0_Conv")
#loc133 = loc("load_/model.5/act/Mul_output_0_Mul_table")
#loc134 = loc("load_/model.6/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc135 = loc("load_/model.6/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc136 = loc("/model.5/act/Mul_output_0_Mul")
#loc137 = loc("load_/model.6/cv1/act/Mul_output_0_Mul_table")
#loc138 = loc("/model.6/cv1/conv/Conv_output_0_Conv")
#loc139 = loc("/model.6/Split_output_0_Split")
#loc140 = loc("/model.6/Split_output_1_Split")
#loc141 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc142 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc143 = loc("/model.6/m.0/cv1/conv/Conv_output_0_Conv")
#loc144 = loc("/model.6/m.0/cv1/act/Mul_output_0_Mul_table")
#loc145 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc146 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc147 = loc("/model.6/m.0/cv2/conv/Conv_output_0_Conv")
#loc148 = loc("load_/model.6/m.0/cv1/conv/Conv_output_0_Conv")
#loc149 = loc("load_/model.6/m.0/cv1/act/Mul_output_0_Mul_table")
#loc150 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc151 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc152 = loc("/model.6/m.0/cv1/act/Mul_output_0_Mul")
#loc153 = loc("/model.6/m.0/cv2/act/Mul_output_0_Mul_table")
#loc154 = loc("/model.6/m.0/Add_output_0_Add")
#loc155 = loc("load_/model.6/m.0/cv2/conv/Conv_output_0_Conv")
#loc156 = loc("load_/model.6/m.0/cv2/act/Mul_output_0_Mul_table")
#loc157 = loc("load_/model.6/Split_output_1_Split")
#loc158 = loc("/model.6/m.0/cv2/act/Mul_output_0_Mul")
#loc159 = loc("/model.6/m.1/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc160 = loc("/model.6/m.1/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc161 = loc("/model.6/m.1/cv1/conv/Conv_output_0_Conv")
#loc162 = loc("/model.6/m.1/cv1/act/Mul_output_0_Mul_table")
#loc163 = loc("/model.6/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc164 = loc("/model.6/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc165 = loc("/model.6/m.1/cv2/conv/Conv_output_0_Conv")
#loc166 = loc("load_/model.6/m.1/cv1/conv/Conv_output_0_Conv")
#loc167 = loc("load_/model.6/m.1/cv1/act/Mul_output_0_Mul_table")
#loc168 = loc("load_/model.6/m.1/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc169 = loc("load_/model.6/m.1/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc170 = loc("/model.6/m.1/cv1/act/Mul_output_0_Mul")
#loc171 = loc("/model.6/m.1/cv2/act/Mul_output_0_Mul_table")
#loc172 = loc("/model.6/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc173 = loc("/model.6/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc174 = loc("/model.6/cv2/act/Mul_output_0_Mul_table")
#loc175 = loc("/model.6/cv2/act/Mul_output_0_Mul")
#loc176 = loc("load_/model.6/m.1/cv2/conv/Conv_output_0_Conv")
#loc177 = loc("load_/model.6/m.1/cv2/act/Mul_output_0_Mul_table")
#loc178 = loc("load_/model.6/m.0/Add_output_0_Add")
#loc179 = loc("/model.6/m.1/cv2/act/Mul_output_0_Mul")
#loc180 = loc("load_/model.6/Split_output_0_Split")
#loc181 = loc("load_/model.6/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc182 = loc("/model.6/m.1/Add_output_0_Add")
#loc183 = loc("load_/model.6/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc184 = loc("/model.6/Concat_output_0_Concat")
#loc185 = loc("load_/model.6/cv2/act/Mul_output_0_Mul_table")
#loc186 = loc("/model.6/cv2/conv/Conv_output_0_Conv")
#loc187 = loc("/model.7/conv/Conv_output_0_Conv_bias_packed")
#loc188 = loc("/model.7/conv/Conv_output_0_Conv_filter_reordered")
#loc189 = loc("/model.7/conv/Conv_output_0_Conv")
#loc190 = loc("/model.7/act/Mul_output_0_Mul_table")
#loc191 = loc("/model.7/act/Mul_output_0_Mul")
#loc192 = loc("/model.8/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc193 = loc("/model.8/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc194 = loc("/model.8/cv1/conv/Conv_output_0_Conv")
#loc195 = loc("/model.8/cv1/act/Mul_output_0_Mul_table")
#loc196 = loc("/model.8/cv1/act/Mul_output_0_Mul")
#loc197 = loc("/model.8/Split_output_0_Split")
#loc198 = loc("/model.8/Split_output_1_Split")
#loc199 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc200 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc201 = loc("/model.8/m.0/cv1/conv/Conv_output_0_Conv")
#loc202 = loc("/model.8/m.0/cv1/act/Mul_output_0_Mul_table")
#loc203 = loc("/model.8/m.0/cv1/act/Mul_output_0_Mul")
#loc204 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc205 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc206 = loc("/model.8/m.0/cv2/conv/Conv_output_0_Conv")
#loc207 = loc("/model.8/m.0/cv2/act/Mul_output_0_Mul_table")
#loc208 = loc("/model.8/Concat_output_0_Concat")
#loc209 = loc("load_/model.8/m.0/cv2/conv/Conv_output_0_Conv")
#loc210 = loc("load_/model.8/m.0/cv2/act/Mul_output_0_Mul_table")
#loc211 = loc("load_/model.8/Split_output_1_Split")
#loc212 = loc("/model.8/m.0/cv2/act/Mul_output_0_Mul")
#loc213 = loc("load_/model.8/Split_output_0_Split")
#loc214 = loc("/model.8/m.0/Add_output_0_Add")
#loc215 = loc("/model.8/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc216 = loc("/model.8/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc217 = loc("/model.8/cv2/conv/Conv_output_0_Conv")
#loc218 = loc("/model.8/cv2/act/Mul_output_0_Mul_table")
#loc219 = loc("/model.9/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc220 = loc("/model.9/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc221 = loc("/model.9/cv1/act/Mul_output_0_Mul_table")
#loc222 = loc("/model.9/cv1/act/Mul_output_0_Mul")
#loc223 = loc("load_/model.8/cv2/conv/Conv_output_0_Conv")
#loc224 = loc("load_/model.8/cv2/act/Mul_output_0_Mul_table")
#loc225 = loc("load_/model.9/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc226 = loc("load_/model.9/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc227 = loc("/model.8/cv2/act/Mul_output_0_Mul")
#loc228 = loc("load_/model.9/cv1/act/Mul_output_0_Mul_table")
#loc229 = loc("/model.9/cv1/conv/Conv_output_0_Conv")
#loc230 = loc("/model.9/m/MaxPool_output_0_MaxPool")
#loc231 = loc("/model.9/m_1/MaxPool_output_0_MaxPool")
#loc232 = loc("/model.9/m_2/MaxPool_output_0_MaxPool")
#loc233 = loc("/model.9/Concat_output_0_Concat")
#loc234 = loc("/model.9/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc235 = loc("/model.9/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc236 = loc("/model.9/cv2/conv/Conv_output_0_Conv")
#loc237 = loc("/model.9/cv2/act/Mul_output_0_Mul_table")
#loc238 = loc("/model.10/Resize_output_0_Resize_filter_reordered")
#loc239 = loc("/model.10/Resize_output_0_Resize_bias_packed")
#loc240 = loc("/model.9/cv2/act/Mul_output_0_Mul")
#loc241 = loc("/model.10/Resize_output_0_Resize")
#loc242 = loc("load_/model.9/cv2/conv/Conv_output_0_Conv")
#loc243 = loc("load_/model.9/cv2/act/Mul_output_0_Mul_table")
#loc244 = loc("load_/model.10/Resize_output_0_Resize_filter_reordered")
#loc245 = loc("load_/model.10/Resize_output_0_Resize_bias_packed")
#loc246 = loc("/model.11/Concat_output_0_Concat")
#loc247 = loc("/model.12/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc248 = loc("/model.12/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc249 = loc("/model.12/cv1/conv/Conv_output_0_Conv")
#loc250 = loc("/model.12/cv1/act/Mul_output_0_Mul_table")
#loc251 = loc("/model.12/cv1/act/Mul_output_0_Mul")
#loc252 = loc("/model.12/Split_output_0_Split")
#loc253 = loc("/model.12/Split_output_1_Split")
#loc254 = loc("/model.12/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc255 = loc("/model.12/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc256 = loc("/model.12/m.0/cv1/conv/Conv_output_0_Conv")
#loc257 = loc("/model.12/m.0/cv1/act/Mul_output_0_Mul_table")
#loc258 = loc("/model.12/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc259 = loc("/model.12/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc260 = loc("/model.12/m.0/cv2/conv/Conv_output_0_Conv")
#loc261 = loc("load_/model.12/m.0/cv1/conv/Conv_output_0_Conv")
#loc262 = loc("load_/model.12/m.0/cv1/act/Mul_output_0_Mul_table")
#loc263 = loc("load_/model.12/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc264 = loc("load_/model.12/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc265 = loc("/model.12/m.0/cv1/act/Mul_output_0_Mul")
#loc266 = loc("/model.12/m.0/cv2/act/Mul_output_0_Mul_table")
#loc267 = loc("/model.12/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc268 = loc("/model.12/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc269 = loc("/model.12/cv2/act/Mul_output_0_Mul_table")
#loc270 = loc("/model.12/cv2/act/Mul_output_0_Mul")
#loc271 = loc("load_/model.12/m.0/cv2/conv/Conv_output_0_Conv")
#loc272 = loc("load_/model.12/m.0/cv2/act/Mul_output_0_Mul_table")
#loc273 = loc("load_/model.12/Split_output_0_Split")
#loc274 = loc("load_/model.12/Split_output_1_Split")
#loc275 = loc("load_/model.12/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc276 = loc("/model.12/m.0/cv2/act/Mul_output_0_Mul")
#loc277 = loc("load_/model.12/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc278 = loc("/model.12/Concat_output_0_Concat")
#loc279 = loc("load_/model.12/cv2/act/Mul_output_0_Mul_table")
#loc280 = loc("/model.12/cv2/conv/Conv_output_0_Conv")
#loc281 = loc("/model.13/Resize_output_0_Resize_filter_reordered")
#loc282 = loc("/model.13/Resize_output_0_Resize_bias_packed")
#loc283 = loc("/model.15/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc284 = loc("/model.15/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc285 = loc("/model.15/cv1/act/Mul_output_0_Mul_table")
#loc286 = loc("/model.15/cv1/act/Mul_output_0_Mul")
#loc287 = loc("load_/model.12/cv2/act/Mul_output_0_Mul")
#loc288 = loc("load_/model.13/Resize_output_0_Resize_filter_reordered")
#loc289 = loc("load_/model.13/Resize_output_0_Resize_bias_packed")
#loc290 = loc("load_/model.4/cv2/act/Mul_output_0_Mul")
#loc291 = loc("/model.13/Resize_output_0_Resize")
#loc292 = loc("load_/model.15/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc293 = loc("load_/model.15/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc294 = loc("/model.14/Concat_output_0_Concat")
#loc295 = loc("load_/model.15/cv1/act/Mul_output_0_Mul_table")
#loc296 = loc("/model.15/cv1/conv/Conv_output_0_Conv")
#loc297 = loc("/model.15/Split_output_0_Split")
#loc298 = loc("/model.15/Split_output_1_Split")
#loc299 = loc("/model.15/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc300 = loc("/model.15/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc301 = loc("/model.15/m.0/cv1/conv/Conv_output_0_Conv")
#loc302 = loc("/model.15/m.0/cv1/act/Mul_output_0_Mul_table")
#loc303 = loc("/model.15/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc304 = loc("/model.15/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc305 = loc("/model.15/m.0/cv2/act/Mul_output_0_Mul_table")
#loc306 = loc("/model.15/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc307 = loc("/model.15/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc308 = loc("/model.15/cv2/act/Mul_output_0_Mul_table")
#loc309 = loc("/model.15/cv2/act/Mul_output_0_Mul")
#loc310 = loc("load_/model.15/m.0/cv1/conv/Conv_output_0_Conv")
#loc311 = loc("load_/model.15/m.0/cv1/act/Mul_output_0_Mul_table")
#loc312 = loc("load_/model.15/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc313 = loc("load_/model.15/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc314 = loc("/model.15/m.0/cv1/act/Mul_output_0_Mul")
#loc315 = loc("load_/model.15/m.0/cv2/act/Mul_output_0_Mul_table")
#loc316 = loc("/model.15/m.0/cv2/conv/Conv_output_0_Conv")
#loc317 = loc("load_/model.15/Split_output_0_Split")
#loc318 = loc("load_/model.15/Split_output_1_Split")
#loc319 = loc("/model.15/m.0/cv2/act/Mul_output_0_Mul")
#loc320 = loc("load_/model.15/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc321 = loc("load_/model.15/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc322 = loc("/model.15/Concat_output_0_Concat")
#loc323 = loc("load_/model.15/cv2/act/Mul_output_0_Mul_table")
#loc324 = loc("/model.15/cv2/conv/Conv_output_0_Conv")
#loc325 = loc("/model.16/conv/Conv_output_0_Conv_bias_packed")
#loc326 = loc("/model.16/conv/Conv_output_0_Conv_filter_reordered")
#loc327 = loc("/model.16/conv/Conv_output_0_Conv")
#loc328 = loc("/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc329 = loc("/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc330 = loc("/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv")
#loc331 = loc("/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc332 = loc("/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc333 = loc("/model.16/act/Mul_output_0_Mul_table")
#loc334 = loc("/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv")
#loc335 = loc("/model.16/act/Mul_output_0_Mul")
#loc336 = loc("load_/model.15/cv2/act/Mul_output_0_Mul")
#loc337 = loc("load_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_filter_reordered")
#loc338 = loc("load_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv_bias_packed")
#loc339 = loc("load_/model.16/conv/Conv_output_0_Conv")
#loc340 = loc("load_/model.16/act/Mul_output_0_Mul_table")
#loc341 = loc("/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul_table")
#loc342 = loc("/model.22/cv3.0/cv3.0.0/act/Mul_output_0_Mul_table")
#loc343 = loc("/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc344 = loc("/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc345 = loc("/model.22/cv3.0/cv3.0.0/act/Mul_output_0_Mul")
#loc346 = loc("/model.17/Concat_output_0_Concat")
#loc347 = loc("/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv")
#loc348 = loc("load_/model.22/cv2.0/cv2.0.0/conv/Conv_output_0_Conv")
#loc349 = loc("load_/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul_table")
#loc350 = loc("load_/model.22/cv3.0/cv3.0.0/conv/Conv_output_0_Conv")
#loc351 = loc("load_/model.22/cv3.0/cv3.0.0/act/Mul_output_0_Mul_table")
#loc352 = loc("/model.22/cv2.0/cv2.0.0/act/Mul_output_0_Mul")
#loc353 = loc("load_/model.16/act/Mul_output_0_Mul")
#loc354 = loc("load_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc355 = loc("load_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc356 = loc("/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv_bias_packed")
#loc357 = loc("/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv_filter_reordered")
#loc358 = loc("/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv")
#loc359 = loc("/model.18/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc360 = loc("/model.18/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc361 = loc("/model.22/cv2.0/cv2.0.1/act/Mul_output_0_Mul_table")
#loc362 = loc("/model.22/cv3.0/cv3.0.1/act/Mul_output_0_Mul_table")
#loc363 = loc("/model.18/cv1/act/Mul_output_0_Mul_table")
#loc364 = loc("/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_bias_packed")
#loc365 = loc("/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_filter_reordered")
#loc366 = loc("/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_bias_packed")
#loc367 = loc("/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_filter_reordered")
#loc368 = loc("/model.18/cv1/act/Mul_output_0_Mul")
#loc369 = loc("/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv")
#loc370 = loc("/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv")
#loc371 = loc("load_/model.18/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc372 = loc("load_/model.17/Concat_output_0_Concat")
#loc373 = loc("load_/model.18/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc374 = loc("load_/model.22/cv2.0/cv2.0.1/conv/Conv_output_0_Conv")
#loc375 = loc("load_/model.22/cv2.0/cv2.0.1/act/Mul_output_0_Mul_table")
#loc376 = loc("/model.18/cv1/conv/Conv_output_0_Conv")
#loc377 = loc("load_/model.22/cv3.0/cv3.0.1/conv/Conv_output_0_Conv")
#loc378 = loc("load_/model.22/cv3.0/cv3.0.1/act/Mul_output_0_Mul_table")
#loc379 = loc("/model.22/cv2.0/cv2.0.1/act/Mul_output_0_Mul")
#loc380 = loc("load_/model.18/cv1/act/Mul_output_0_Mul_table")
#loc381 = loc("/model.22/cv3.0/cv3.0.1/act/Mul_output_0_Mul")
#loc382 = loc("load_/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_filter_reordered")
#loc383 = loc("load_/model.22/cv2.0/cv2.0.2/Conv_output_0_Conv_bias_packed")
#loc384 = loc("load_/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_filter_reordered")
#loc385 = loc("load_/model.22/cv3.0/cv3.0.2/Conv_output_0_Conv_bias_packed")
#loc386 = loc("/model.18/Split_output_0_Split")
#loc387 = loc("/model.18/Split_output_1_Split")
#loc388 = loc("/model.22/Concat_output_0_Concat")
#loc389 = loc("/model.18/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc390 = loc("/model.18/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc391 = loc("/model.18/m.0/cv1/conv/Conv_output_0_Conv")
#loc392 = loc("/model.22/Reshape_output_0_Reshape")
#loc393 = loc("/model.18/m.0/cv1/act/Mul_output_0_Mul_table")
#loc394 = loc("/model.18/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc395 = loc("/model.18/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc396 = loc("/model.18/m.0/cv2/conv/Conv_output_0_Conv")
#loc397 = loc("load_/model.18/m.0/cv1/conv/Conv_output_0_Conv")
#loc398 = loc("load_/model.18/m.0/cv1/act/Mul_output_0_Mul_table")
#loc399 = loc("load_/model.18/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc400 = loc("load_/model.18/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc401 = loc("/model.18/m.0/cv1/act/Mul_output_0_Mul")
#loc402 = loc("/model.18/m.0/cv2/act/Mul_output_0_Mul_table")
#loc403 = loc("/model.18/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc404 = loc("/model.18/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc405 = loc("/model.18/cv2/act/Mul_output_0_Mul_table")
#loc406 = loc("/model.18/cv2/act/Mul_output_0_Mul")
#loc407 = loc("load_/model.18/m.0/cv2/conv/Conv_output_0_Conv")
#loc408 = loc("load_/model.18/m.0/cv2/act/Mul_output_0_Mul_table")
#loc409 = loc("load_/model.18/Split_output_0_Split")
#loc410 = loc("load_/model.18/Split_output_1_Split")
#loc411 = loc("/model.18/m.0/cv2/act/Mul_output_0_Mul")
#loc412 = loc("load_/model.18/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc413 = loc("load_/model.18/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc414 = loc("/model.18/Concat_output_0_Concat")
#loc415 = loc("load_/model.18/cv2/act/Mul_output_0_Mul_table")
#loc416 = loc("/model.18/cv2/conv/Conv_output_0_Conv")
#loc417 = loc("/model.19/conv/Conv_output_0_Conv_bias_packed")
#loc418 = loc("/model.19/conv/Conv_output_0_Conv_filter_reordered")
#loc419 = loc("/model.19/conv/Conv_output_0_Conv")
#loc420 = loc("/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc421 = loc("/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc422 = loc("/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv")
#loc423 = loc("/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv_bias_packed")
#loc424 = loc("/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv_filter_reordered")
#loc425 = loc("/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv")
#loc426 = loc("/model.19/act/Mul_output_0_Mul_table")
#loc427 = loc("/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul_table")
#loc428 = loc("/model.22/cv3.1/cv3.1.0/act/Mul_output_0_Mul_table")
#loc429 = loc("/model.19/act/Mul_output_0_Mul")
#loc430 = loc("/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul")
#loc431 = loc("/model.22/cv3.1/cv3.1.0/act/Mul_output_0_Mul")
#loc432 = loc("load_/model.19/conv/Conv_output_0_Conv")
#loc433 = loc("load_/model.19/act/Mul_output_0_Mul_table")
#loc434 = loc("load_/model.22/cv2.1/cv2.1.0/conv/Conv_output_0_Conv")
#loc435 = loc("load_/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul_table")
#loc436 = loc("load_/model.22/cv3.1/cv3.1.0/conv/Conv_output_0_Conv")
#loc437 = loc("load_/model.22/cv3.1/cv3.1.0/act/Mul_output_0_Mul_table")
#loc438 = loc("/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc439 = loc("/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc440 = loc("/model.20/Concat_output_0_Concat")
#loc441 = loc("/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv")
#loc442 = loc("load_/model.19/act/Mul_output_0_Mul")
#loc443 = loc("load_/model.9/cv2/act/Mul_output_0_Mul")
#loc444 = loc("load_/model.22/cv2.1/cv2.1.0/act/Mul_output_0_Mul")
#loc445 = loc("load_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc446 = loc("load_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc447 = loc("/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv_bias_packed")
#loc448 = loc("/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv_filter_reordered")
#loc449 = loc("/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv")
#loc450 = loc("/model.21/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc451 = loc("/model.21/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc452 = loc("/model.21/cv1/conv/Conv_output_0_Conv")
#loc453 = loc("/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul_table")
#loc454 = loc("/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul_table")
#loc455 = loc("/model.21/cv1/act/Mul_output_0_Mul_table")
#loc456 = loc("/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul")
#loc457 = loc("/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul")
#loc458 = loc("/model.21/cv1/act/Mul_output_0_Mul")
#loc459 = loc("load_/model.22/cv2.1/cv2.1.1/conv/Conv_output_0_Conv")
#loc460 = loc("load_/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul_table")
#loc461 = loc("load_/model.22/cv3.1/cv3.1.1/conv/Conv_output_0_Conv")
#loc462 = loc("load_/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul_table")
#loc463 = loc("load_/model.21/cv1/conv/Conv_output_0_Conv")
#loc464 = loc("load_/model.21/cv1/act/Mul_output_0_Mul_table")
#loc465 = loc("/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_bias_packed")
#loc466 = loc("/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_filter_reordered")
#loc467 = loc("/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_bias_packed")
#loc468 = loc("/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_filter_reordered")
#loc469 = loc("/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv")
#loc470 = loc("/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv")
#loc471 = loc("load_/model.22/cv2.1/cv2.1.1/act/Mul_output_0_Mul")
#loc472 = loc("load_/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_filter_reordered")
#loc473 = loc("load_/model.22/cv2.1/cv2.1.2/Conv_output_0_Conv_bias_packed")
#loc474 = loc("load_/model.22/cv3.1/cv3.1.1/act/Mul_output_0_Mul")
#loc475 = loc("load_/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_filter_reordered")
#loc476 = loc("load_/model.22/cv3.1/cv3.1.2/Conv_output_0_Conv_bias_packed")
#loc477 = loc("/model.21/Split_output_0_Split")
#loc478 = loc("/model.21/Split_output_1_Split")
#loc479 = loc("/model.22/Concat_1_output_0_Concat")
#loc480 = loc("/model.21/m.0/cv1/conv/Conv_output_0_Conv_bias_packed")
#loc481 = loc("/model.21/m.0/cv1/conv/Conv_output_0_Conv_filter_reordered")
#loc482 = loc("/model.21/m.0/cv1/conv/Conv_output_0_Conv")
#loc483 = loc("/model.22/Reshape_1_output_0_Reshape")
#loc484 = loc("/model.21/m.0/cv1/act/Mul_output_0_Mul_table")
#loc485 = loc("/model.21/m.0/cv1/act/Mul_output_0_Mul")
#loc486 = loc("/model.21/m.0/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc487 = loc("/model.21/m.0/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc488 = loc("/model.21/m.0/cv2/conv/Conv_output_0_Conv")
#loc489 = loc("/model.21/m.0/cv2/act/Mul_output_0_Mul_table")
#loc490 = loc("/model.21/Concat_output_0_Concat")
#loc491 = loc("load_/model.21/m.0/cv2/conv/Conv_output_0_Conv")
#loc492 = loc("load_/model.21/m.0/cv2/act/Mul_output_0_Mul_table")
#loc493 = loc("load_/model.21/Split_output_0_Split")
#loc494 = loc("load_/model.21/Split_output_1_Split")
#loc495 = loc("/model.21/m.0/cv2/act/Mul_output_0_Mul")
#loc496 = loc("/model.21/cv2/conv/Conv_output_0_Conv_bias_packed")
#loc497 = loc("/model.21/cv2/conv/Conv_output_0_Conv_filter_reordered")
#loc498 = loc("/model.21/cv2/conv/Conv_output_0_Conv")
#loc499 = loc("/model.21/cv2/act/Mul_output_0_Mul_table")
#loc500 = loc("/model.21/cv2/act/Mul_output_0_Mul")
#loc501 = loc("/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc502 = loc("/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc503 = loc("/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv")
#loc504 = loc("/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv_bias_packed")
#loc505 = loc("/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv_filter_reordered")
#loc506 = loc("/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv")
#loc507 = loc("/model.22/cv2.2/cv2.2.0/act/Mul_output_0_Mul_table")
#loc508 = loc("/model.22/cv3.2/cv3.2.0/act/Mul_output_0_Mul_table")
#loc509 = loc("/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc510 = loc("/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc511 = loc("/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc512 = loc("/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc513 = loc("/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv")
#loc514 = loc("/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv")
#loc515 = loc("load_/model.22/cv2.2/cv2.2.0/conv/Conv_output_0_Conv")
#loc516 = loc("load_/model.22/cv2.2/cv2.2.0/act/Mul_output_0_Mul_table")
#loc517 = loc("load_/model.22/cv3.2/cv3.2.0/conv/Conv_output_0_Conv")
#loc518 = loc("load_/model.22/cv3.2/cv3.2.0/act/Mul_output_0_Mul_table")
#loc519 = loc("load_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc520 = loc("/model.22/cv2.2/cv2.2.0/act/Mul_output_0_Mul")
#loc521 = loc("load_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc522 = loc("/model.22/cv3.2/cv3.2.0/act/Mul_output_0_Mul")
#loc523 = loc("load_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_filter_reordered")
#loc524 = loc("load_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv_bias_packed")
#loc525 = loc("/model.22/cv2.2/cv2.2.1/act/Mul_output_0_Mul_table")
#loc526 = loc("/model.22/cv3.2/cv3.2.1/act/Mul_output_0_Mul_table")
#loc527 = loc("/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_bias_packed")
#loc528 = loc("/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_filter_reordered")
#loc529 = loc("/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_bias_packed")
#loc530 = loc("/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_filter_reordered")
#loc531 = loc("/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv")
#loc532 = loc("/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv")
#loc533 = loc("load_/model.22/cv2.2/cv2.2.1/conv/Conv_output_0_Conv")
#loc534 = loc("load_/model.22/cv2.2/cv2.2.1/act/Mul_output_0_Mul_table")
#loc535 = loc("load_/model.22/cv3.2/cv3.2.1/conv/Conv_output_0_Conv")
#loc536 = loc("load_/model.22/cv3.2/cv3.2.1/act/Mul_output_0_Mul_table")
#loc537 = loc("/model.22/cv2.2/cv2.2.1/act/Mul_output_0_Mul")
#loc538 = loc("load_/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_filter_reordered")
#loc539 = loc("load_/model.22/cv2.2/cv2.2.2/Conv_output_0_Conv_bias_packed")
#loc540 = loc("/model.22/cv3.2/cv3.2.1/act/Mul_output_0_Mul")
#loc541 = loc("load_/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_filter_reordered")
#loc542 = loc("load_/model.22/cv3.2/cv3.2.2/Conv_output_0_Conv_bias_packed")
#loc543 = loc("/model.22/Concat_2_output_0_Concat")
#loc544 = loc("/model.22/Reshape_2_output_0_Reshape")
#loc545 = loc("/model.22/Concat_3_output_0_Concat")
#loc546 = loc("/model.22/Split_output_1_Split")
#loc547 = loc("/model.22/Sigmoid_output_0_Sigmoid_table")
#loc548 = loc("/model.22/Sigmoid_output_0_Sigmoid")
#loc549 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_slope_table_bf16")
#loc550 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_slope_slope_table_bf16")
#loc551 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_pow_table_bf16")
#loc552 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/Softmax_output_0_Softmax_pow_mantissa_table_bf16")
#loc553 = loc("/model.22/dfl/conv/Conv_output_0_Conv_bias_packed")
#loc554 = loc("/model.22/dfl/conv/Conv_output_0_Conv_filter_reordered")
#loc555 = loc("/model.22/Split_output_0_Split")
#loc556 = loc("/model.22/dfl/Reshape_output_0_Reshape")
#loc557 = loc("/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16_/model.22/dfl/Transpose_output_0_Transpose")
#loc558 = loc("/model.22/dfl/Softmax_output_0_Softmax_/model.22/dfl/Transpose_output_0_Transpose_/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16")
#loc559 = loc("/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/conv/Conv_output_0_Conv_si8_/model.22/dfl/Transpose_output_0_Transpose_/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16_/model.22/dfl/Softmax_output_0_Softmax")
#loc560 = loc("/model.22/dfl/Transpose_output_0_Transpose_/model.22/dfl/Transpose_output_0_Transpose/model.22/dfl/Softmax_output_0_Softmax_bf16_/model.22/dfl/Softmax_output_0_Softmax_/model.22/dfl/Softmax_output_0_Softmax/model.22/dfl/conv/Conv_output_0_Conv_si8")
#loc561 = loc("/model.22/dfl/conv/Conv_output_0_Conv")
#loc562 = loc("/model.22/dfl/conv/Conv_output_0_Conv_f32")
#loc563 = loc("/model.22/Sigmoid_output_0_Sigmoid_f32")
#loc564 = loc(fused[#loc240, #loc241])
#loc565 = loc(fused[#loc334, #loc335])
#loc566 = loc(fused[#loc345, #loc346, #loc347])
#loc567 = loc(fused[#loc368, #loc369, #loc370])
#loc568 = loc(fused[#loc429, #loc430, #loc431])
#loc569 = loc(fused[#loc440, #loc441])
#loc570 = loc(fused[#loc456, #loc457, #loc458])
#loc571 = loc(fused[#loc469, #loc470])
#loc572 = loc(fused[#loc513, #loc514])
#loc573 = loc(fused[#loc531, #loc532])

